# OctoLLM Unraid Environment Configuration
# Copy this file to .env.unraid and customize for your deployment
# Never commit .env.unraid to version control!

# ============================================================================
# Network Configuration
# ============================================================================

# Unraid host IP address (used for service URLs)
HOST_IP=192.168.4.6

# ============================================================================
# PostgreSQL Configuration
# ============================================================================

POSTGRES_DB=octollm
POSTGRES_USER=octollm
# SECURITY: Change this to a strong random password!
# Generate with: openssl rand -base64 32
POSTGRES_PASSWORD=CHANGE_ME_POSTGRES_PASSWORD_HERE

# ============================================================================
# Redis Configuration
# ============================================================================

# SECURITY: Change this to a strong random password!
# Generate with: openssl rand -base64 32
REDIS_PASSWORD=CHANGE_ME_REDIS_PASSWORD_HERE

# ============================================================================
# Qdrant Vector Database Configuration
# ============================================================================

# SECURITY: Change this to a strong random API key!
# Generate with: openssl rand -base64 32
QDRANT_API_KEY=CHANGE_ME_QDRANT_API_KEY_HERE
QDRANT_LOG_LEVEL=INFO

# Vector embedding dimension (depends on embedding model)
# nomic-embed-text: 768
# text-embedding-3-small: 1536
# text-embedding-3-large: 3072
EMBEDDING_DIMENSION=768

# ============================================================================
# Local LLM Configuration (Ollama with Tesla P40)
# ============================================================================

# Prefer local GPU-accelerated LLMs over cloud APIs
ENABLE_GPU_INFERENCE=true
PREFER_LOCAL_LLM=true

# Ollama performance tuning
OLLAMA_NUM_PARALLEL=4              # Number of parallel requests
OLLAMA_MAX_LOADED_MODELS=3         # Max models in VRAM simultaneously
OLLAMA_KEEP_ALIVE=5m               # Model unload timeout

# Primary models (pulled during setup)
OLLAMA_PRIMARY_MODEL=llama3.1:8b
OLLAMA_FALLBACK_MODEL=mixtral:8x7b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Arm-specific models
OLLAMA_PLANNER_MODEL=llama3.1:8b
OLLAMA_CODER_MODEL=codellama:13b
OLLAMA_JUDGE_MODEL=llama3.1:8b

# ============================================================================
# Cloud LLM APIs (Optional - Fallback Only)
# ============================================================================

# OpenAI API (optional, for fallback when local LLM unavailable)
# Get key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# OpenAI model selection
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_PLANNER_MODEL=gpt-3.5-turbo
OPENAI_CODER_MODEL=gpt-4-turbo-preview
OPENAI_JUDGE_MODEL=gpt-3.5-turbo
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Anthropic API (optional, for advanced reasoning)
# Get key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=

# Anthropic model selection
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# ============================================================================
# Orchestrator Configuration
# ============================================================================

LOG_LEVEL=INFO
ENVIRONMENT=development

# Task execution limits
MAX_PARALLEL_ARMS=5
TASK_TIMEOUT=300                   # seconds
CACHE_TTL=3600                     # seconds

# Advanced features
ENABLE_SWARM_DECISIONS=false       # Enable multi-agent consensus

# Observability
PROMETHEUS_METRICS_ENABLED=true
PROMETHEUS_PORT=8000
JAEGER_ENABLED=false
JAEGER_AGENT_HOST=jaeger
JAEGER_AGENT_PORT=6831

# ============================================================================
# Reflex Layer Configuration
# ============================================================================

REFLEX_HOST=0.0.0.0
REFLEX_PORT=3001
REFLEX_CACHE_TTL=3600              # seconds
REFLEX_RATE_LIMIT_PER_MINUTE=1000

# PII Detection
PII_DETECTION_ENABLED=true
PII_AUTO_REDACT=true

# Rust logging
RUST_LOG=info
RUST_BACKTRACE=0                   # Set to 1 for debugging

# ============================================================================
# Arm Configuration
# ============================================================================

# Planner Arm
PLANNER_PORT=6001

# Executor Arm (sandboxed command execution)
EXECUTOR_PORT=6002
EXECUTOR_SANDBOX_ENABLED=true
EXECUTOR_COMMAND_TIMEOUT=30        # seconds
EXECUTOR_MAX_CONCURRENT=5

# Retriever Arm
RETRIEVER_PORT=6003
RETRIEVER_MAX_RESULTS=10

# Coder Arm
CODER_PORT=6004
CODER_MAX_LENGTH=50000             # characters
CODER_SYNTAX_CHECK=true

# Judge Arm
JUDGE_PORT=6005
JUDGE_STRICT_MODE=true

# Safety Guardian Arm
SAFETY_GUARDIAN_PORT=6006
CONTENT_FILTER_ENABLED=true
INJECTION_DETECTION_ENABLED=true
BLOCKLIST_ENABLED=true

# ============================================================================
# Monitoring Configuration
# ============================================================================

# Grafana admin credentials
GRAFANA_ADMIN_USER=admin
# SECURITY: Change this to a strong password!
GRAFANA_ADMIN_PASSWORD=CHANGE_ME_GRAFANA_PASSWORD_HERE

# Grafana plugins (comma-separated)
GRAFANA_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel

# ============================================================================
# Resource Limits (Docker Compose)
# ============================================================================

# These are soft limits - adjust based on your workload
# Dell PowerEdge R730xd has 504GB RAM, 64 threads, Tesla P40 24GB VRAM

# Total allocation: ~48GB RAM, 38 CPU cores, 24GB VRAM
# Leaves: ~450GB RAM, 26 cores for other Unraid services

# ============================================================================
# Feature Flags
# ============================================================================

# Enable/disable optional features
ENABLE_MONITORING=true
ENABLE_TRACING=false

# ============================================================================
# Security Settings
# ============================================================================

# TLS/SSL (for production deployment behind reverse proxy)
ENABLE_TLS=false
TLS_CERT_PATH=/mnt/user/appdata/octollm/certs/cert.pem
TLS_KEY_PATH=/mnt/user/appdata/octollm/certs/key.pem

# CORS settings (for web frontends)
CORS_ALLOWED_ORIGINS=http://192.168.4.6:3000,http://localhost:3000

# ============================================================================
# Development/Debug Settings
# ============================================================================

# Enable verbose logging for debugging
DEBUG_MODE=false

# Reload on code changes (development only)
HOT_RELOAD_ENABLED=false

# ============================================================================
# Cost Estimation (Monthly)
# ============================================================================

# With PREFER_LOCAL_LLM=true and Tesla P40 GPU:
# - OpenAI API: $0/month (fallback only)
# - Anthropic API: $0/month (optional)
# - Total monthly cost: ~$0 (electricity only!)
#
# With PREFER_LOCAL_LLM=false (cloud APIs):
# - OpenAI API: $50-200/month (typical dev usage)
# - Anthropic API: $100-500/month (if enabled)
# - Total monthly cost: $150-700/month
#
# SAVINGS: Using local GPU saves $150-700/month in LLM API costs!

# ============================================================================
# Hardware Utilization (Expected)
# ============================================================================

# CPU: 38/64 cores (59% utilization)
# RAM: 48GB/504GB (9.5% utilization)
# GPU: 24GB VRAM (100% utilization during inference)
# Network: <1% of 4Gbps aggregate (bond0)
# Disk I/O: Moderate (databases on cache SSD)

# ============================================================================
# Port Mapping Reference
# ============================================================================

# Core Services:
# 3000  - Orchestrator API (main entry point)
# 3001  - Reflex Layer API
#
# Infrastructure:
# 3010  - PostgreSQL (internal use)
# 3011  - Redis (internal use)
# 3012  - Qdrant HTTP API
# 3013  - Qdrant gRPC API
# 3014  - Ollama API
#
# Arms:
# 6001  - Planner Arm
# 6002  - Executor Arm
# 6003  - Retriever Arm
# 6004  - Coder Arm
# 6005  - Judge Arm
# 6006  - Safety Guardian Arm
#
# Monitoring:
# 3030  - Grafana UI
# 3100  - Loki (logs)
# 8080  - cAdvisor (container metrics)
# 9090  - Prometheus
# 9100  - Node Exporter
# 9121  - Redis Exporter
# 9187  - PostgreSQL Exporter
# 9400  - NVIDIA DCGM Exporter (GPU metrics)

# ============================================================================
# Backup Configuration
# ============================================================================

# Backup schedule (for backup-data.sh script)
BACKUP_ENABLED=true
BACKUP_RETENTION_DAYS=30
BACKUP_DESTINATION=/mnt/user/backups/octollm

# ============================================================================
# Notes
# ============================================================================

# 1. Replace all CHANGE_ME_* values before starting services
# 2. Generate secure passwords with: openssl rand -base64 32
# 3. For production, set ENVIRONMENT=production
# 4. Access Grafana at: http://192.168.4.6:3030
# 5. Access Orchestrator API docs at: http://192.168.4.6:3000/docs
# 6. Monitor GPU with: nvidia-smi or http://192.168.4.6:3030 (Grafana)
