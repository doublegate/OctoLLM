<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Troubleshooting Playbooks - OctoLLM Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Distributed AI Architecture for Offensive Security and Developer Tooling - Comprehensive technical documentation covering architecture, API, development, operations, and security.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">OctoLLM Documentation</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/doublegate/OctoLLM" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/doublegate/OctoLLM/edit/main/docs/src/operations/troubleshooting-playbooks.md" title="Suggest an edit" aria-label="Suggest an edit" rel="edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="troubleshooting-playbooks"><a class="header" href="#troubleshooting-playbooks">Troubleshooting Playbooks</a></h1>
<p><strong>Purpose</strong>: Step-by-step procedures for diagnosing and resolving common OctoLLM issues
<strong>Audience</strong>: Operations engineers, SREs, on-call responders
<strong>Prerequisites</strong>: Access to logs, metrics, and deployment environment</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>This document provides systematic troubleshooting procedures for common OctoLLM issues. Each playbook follows a structured format:</p>
<ol>
<li><strong>Symptoms</strong> - How to recognize the problem</li>
<li><strong>Diagnosis</strong> - Steps to identify root cause</li>
<li><strong>Resolution</strong> - How to fix the issue</li>
<li><strong>Prevention</strong> - How to avoid recurrence</li>
</ol>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#service-unavailable">Service Unavailable</a></li>
<li><a href="#high-latency">High Latency</a></li>
<li><a href="#database-connection-issues">Database Connection Issues</a></li>
<li><a href="#memory-leaks">Memory Leaks</a></li>
<li><a href="#task-routing-failures">Task Routing Failures</a></li>
<li><a href="#llm-api-failures">LLM API Failures</a></li>
<li><a href="#cache-performance-issues">Cache Performance Issues</a></li>
<li><a href="#resource-exhaustion">Resource Exhaustion</a></li>
<li><a href="#security-violations">Security Violations</a></li>
<li><a href="#data-corruption">Data Corruption</a></li>
</ol>
<hr />
<h2 id="service-unavailable"><a class="header" href="#service-unavailable">Service Unavailable</a></h2>
<h3 id="symptoms"><a class="header" href="#symptoms">Symptoms</a></h3>
<ul>
<li>HTTP 503 responses from API</li>
<li>Health check failures</li>
<li>No response from service endpoints</li>
<li>Alert: <code>ServiceDown</code> or <code>ArmDown</code></li>
</ul>
<h3 id="diagnosis"><a class="header" href="#diagnosis">Diagnosis</a></h3>
<p><strong>Step 1: Check service status</strong></p>
<pre><code class="language-bash"># Docker Compose
docker compose ps

# Kubernetes
kubectl get pods -n octollm
kubectl describe pod &lt;pod-name&gt; -n octollm
</code></pre>
<p><strong>Step 2: Check container logs</strong></p>
<pre><code class="language-bash"># Docker Compose
docker compose logs --tail=100 orchestrator

# Kubernetes
kubectl logs &lt;pod-name&gt; -n octollm --tail=100
</code></pre>
<p><strong>Step 3: Check resource usage</strong></p>
<pre><code class="language-bash"># Docker
docker stats

# Kubernetes
kubectl top pods -n octollm
kubectl describe node &lt;node-name&gt;
</code></pre>
<p><strong>Step 4: Check dependencies</strong></p>
<pre><code class="language-bash"># Verify database connections
docker compose exec orchestrator nc -zv postgres 5432
docker compose exec orchestrator nc -zv redis 6379
docker compose exec orchestrator nc -zv qdrant 6333

# Check database health
docker compose exec postgres pg_isready -U octollm
docker compose exec redis redis-cli ping
</code></pre>
<h3 id="resolution"><a class="header" href="#resolution">Resolution</a></h3>
<p><strong>Scenario A: Container crashed</strong></p>
<pre><code class="language-bash"># Check exit code and restart
docker compose ps
docker compose logs &lt;service&gt;
docker compose restart &lt;service&gt;

# Kubernetes
kubectl get pods -n octollm
kubectl logs &lt;pod-name&gt; -n octollm --previous
kubectl delete pod &lt;pod-name&gt; -n octollm  # Force restart
</code></pre>
<p><strong>Scenario B: Out of memory</strong></p>
<pre><code class="language-bash"># Increase memory limits
# In .env for Docker Compose:
ORCHESTRATOR_MEMORY_LIMIT=8g

# In Kubernetes:
kubectl edit deployment orchestrator -n octollm
# Update resources.limits.memory to higher value

# Restart service
docker compose up -d orchestrator
# or
kubectl rollout restart deployment orchestrator -n octollm
</code></pre>
<p><strong>Scenario C: Database connection failure</strong></p>
<pre><code class="language-bash"># Restart database
docker compose restart postgres

# Verify connectivity
docker compose exec orchestrator ping postgres

# Check network
docker network inspect octollm_octollm-network

# Kubernetes: Check network policies
kubectl get networkpolicies -n octollm
</code></pre>
<p><strong>Scenario D: Configuration error</strong></p>
<pre><code class="language-bash"># Validate environment variables
docker compose config

# Check configuration in running container
docker compose exec orchestrator env | grep POSTGRES

# Fix configuration in .env and restart
docker compose up -d orchestrator
</code></pre>
<h3 id="prevention"><a class="header" href="#prevention">Prevention</a></h3>
<ol>
<li><strong>Set up health checks</strong>: Ensure all services have proper liveness/readiness probes</li>
<li><strong>Resource reservations</strong>: Set CPU/memory requests and limits</li>
<li><strong>Monitoring</strong>: Alert on service availability (ServiceDown alert)</li>
<li><strong>Auto-restart</strong>: Use <code>restart: unless-stopped</code> in Docker Compose</li>
<li><strong>Pod Disruption Budgets</strong>: Ensure minimum replicas in Kubernetes</li>
</ol>
<hr />
<h2 id="high-latency"><a class="header" href="#high-latency">High Latency</a></h2>
<h3 id="symptoms-1"><a class="header" href="#symptoms-1">Symptoms</a></h3>
<ul>
<li>Slow API responses (&gt;5 seconds)</li>
<li>Task processing delays</li>
<li>Timeouts from clients</li>
<li>Alert: <code>HighRequestLatency</code></li>
</ul>
<h3 id="diagnosis-1"><a class="header" href="#diagnosis-1">Diagnosis</a></h3>
<p><strong>Step 1: Identify slow endpoints</strong></p>
<pre><code class="language-bash"># Query Prometheus for P95 latency by endpoint
curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))'

# Check Grafana dashboard for latency breakdown
</code></pre>
<p><strong>Step 2: Check resource utilization</strong></p>
<pre><code class="language-bash"># CPU usage
docker stats
# or
kubectl top pods -n octollm

# Memory pressure
free -h
# or
kubectl describe node &lt;node-name&gt;
</code></pre>
<p><strong>Step 3: Identify bottlenecks</strong></p>
<pre><code class="language-bash"># Check database query performance
docker compose exec postgres psql -U octollm -c "
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;"

# Check Redis performance
docker compose exec redis redis-cli --latency

# Check LLM API latency
# Review metrics: llm_api_duration_seconds
</code></pre>
<p><strong>Step 4: Profile application</strong></p>
<pre><code class="language-bash"># Python profiling (add to orchestrator temporarily)
python -m cProfile -o profile.stats app/main.py

# View profile
python -m pstats profile.stats
&gt; sort cumtime
&gt; stats 20
</code></pre>
<h3 id="resolution-1"><a class="header" href="#resolution-1">Resolution</a></h3>
<p><strong>Scenario A: Database slow queries</strong></p>
<pre><code class="language-sql">-- Add missing indexes
CREATE INDEX CONCURRENTLY idx_tasks_created_at ON tasks(created_at);
CREATE INDEX CONCURRENTLY idx_entities_type ON entities(entity_type);

-- Optimize frequently accessed queries
EXPLAIN ANALYZE SELECT * FROM tasks WHERE status = 'pending';

-- Update statistics
ANALYZE tasks;
VACUUM ANALYZE;
</code></pre>
<p><strong>Scenario B: LLM API latency</strong></p>
<pre><code class="language-python"># Implement request batching
# In orchestrator/app/services/llm_client.py

async def batch_requests(requests: List[Request]) -&gt; List[Response]:
    """Batch multiple LLM requests into single API call"""
    combined_prompt = "\n---\n".join([r.prompt for r in requests])

    response = await self.client.chat.completions.create(
        model=self.model,
        messages=[{"role": "user", "content": combined_prompt}]
    )

    # Split and return individual responses
    return parse_batch_response(response)
</code></pre>
<pre><code class="language-python"># Implement caching for repeated queries
from functools import lru_cache
import hashlib

async def get_llm_response(prompt: str) -&gt; str:
    # Check Redis cache first
    cache_key = f"llm:{hashlib.md5(prompt.encode()).hexdigest()}"
    cached = await redis_client.get(cache_key)

    if cached:
        cache_hits_total.labels(cache_type="llm").inc()
        return cached

    # Make API call
    response = await llm_client.generate(prompt)

    # Cache for 1 hour
    await redis_client.setex(cache_key, 3600, response)

    return response
</code></pre>
<p><strong>Scenario C: Resource contention</strong></p>
<pre><code class="language-bash"># Scale horizontally (Kubernetes)
kubectl scale deployment orchestrator --replicas=4 -n octollm

# Docker Compose: Update docker-compose.yml
services:
  orchestrator:
    deploy:
      replicas: 3

# Scale vertically: Increase CPU/memory
kubectl edit deployment orchestrator -n octollm
# Update resources.limits
</code></pre>
<p><strong>Scenario D: Network latency</strong></p>
<pre><code class="language-bash"># Check network latency between services
docker compose exec orchestrator time curl -s http://planner-arm:8100/health

# Optimize service communication
# Use connection pooling
# Implement circuit breakers
# Add retry logic with exponential backoff
</code></pre>
<h3 id="prevention-1"><a class="header" href="#prevention-1">Prevention</a></h3>
<ol>
<li><strong>Connection pooling</strong>: Configure database connection pools</li>
<li><strong>Caching strategy</strong>: Cache frequently accessed data</li>
<li><strong>Query optimization</strong>: Add indexes, optimize N+1 queries</li>
<li><strong>Request batching</strong>: Batch LLM API requests</li>
<li><strong>Rate limiting</strong>: Prevent resource exhaustion</li>
<li><strong>Horizontal scaling</strong>: Use auto-scaling based on metrics</li>
</ol>
<hr />
<h2 id="database-connection-issues"><a class="header" href="#database-connection-issues">Database Connection Issues</a></h2>
<h3 id="symptoms-2"><a class="header" href="#symptoms-2">Symptoms</a></h3>
<ul>
<li>Connection refused errors</li>
<li>Connection timeout</li>
<li><code>psycopg2.OperationalError</code> or <code>ConnectionError</code></li>
<li>Alert: <code>PostgreSQLDown</code> or <code>HighDatabaseConnections</code></li>
</ul>
<h3 id="diagnosis-2"><a class="header" href="#diagnosis-2">Diagnosis</a></h3>
<p><strong>Step 1: Verify database is running</strong></p>
<pre><code class="language-bash"># Check database status
docker compose ps postgres
docker compose exec postgres pg_isready -U octollm

# Kubernetes
kubectl get pods -l app=postgres -n octollm
kubectl logs -l app=postgres -n octollm
</code></pre>
<p><strong>Step 2: Check connection limits</strong></p>
<pre><code class="language-sql">-- Check current connections
docker compose exec postgres psql -U octollm -c "
SELECT count(*) as current_connections,
       (SELECT setting::int FROM pg_settings WHERE name='max_connections') as max_connections
FROM pg_stat_activity;"

-- View active connections
docker compose exec postgres psql -U octollm -c "
SELECT pid, usename, application_name, client_addr, state, query
FROM pg_stat_activity
WHERE state != 'idle';"
</code></pre>
<p><strong>Step 3: Test connectivity</strong></p>
<pre><code class="language-bash"># From orchestrator container
docker compose exec orchestrator nc -zv postgres 5432

# Manual connection test
docker compose exec orchestrator psql -h postgres -U octollm -d octollm -c "SELECT 1;"
</code></pre>
<p><strong>Step 4: Check network configuration</strong></p>
<pre><code class="language-bash"># Docker network
docker network inspect octollm_octollm-network

# Kubernetes network policy
kubectl describe networkpolicy -n octollm
</code></pre>
<h3 id="resolution-2"><a class="header" href="#resolution-2">Resolution</a></h3>
<p><strong>Scenario A: Connection pool exhausted</strong></p>
<pre><code class="language-python"># Increase pool size in orchestrator/app/database/connection.py

from sqlalchemy.ext.asyncio import create_async_engine

engine = create_async_engine(
    DATABASE_URL,
    pool_size=20,          # Increased from 5
    max_overflow=40,       # Increased from 10
    pool_timeout=30,
    pool_recycle=3600,
    pool_pre_ping=True,    # Verify connections before use
)
</code></pre>
<p><strong>Scenario B: Too many open connections</strong></p>
<pre><code class="language-sql">-- Increase max_connections in PostgreSQL
docker compose exec postgres psql -U octollm -c "
ALTER SYSTEM SET max_connections = 200;
SELECT pg_reload_conf();"

-- Or update postgresql.conf
echo "max_connections = 200" &gt;&gt; data/postgres/postgresql.conf
docker compose restart postgres
</code></pre>
<p><strong>Scenario C: Connection leak</strong></p>
<pre><code class="language-python"># Fix connection leaks - always use context managers

# Bad (connection leak):
conn = await pool.acquire()
result = await conn.fetch("SELECT * FROM tasks")
# conn never released!

# Good (automatic cleanup):
async with pool.acquire() as conn:
    result = await conn.fetch("SELECT * FROM tasks")
    # conn automatically released
</code></pre>
<p><strong>Scenario D: Network partition</strong></p>
<pre><code class="language-bash"># Docker: Recreate network
docker compose down
docker network prune
docker compose up -d

# Kubernetes: Check DNS resolution
kubectl run -it --rm debug --image=busybox --restart=Never -- nslookup postgres.octollm.svc.cluster.local

# Verify network policies allow traffic
kubectl get networkpolicies -n octollm
</code></pre>
<h3 id="prevention-2"><a class="header" href="#prevention-2">Prevention</a></h3>
<ol>
<li><strong>Connection pooling</strong>: Always use connection pools</li>
<li><strong>Context managers</strong>: Use <code>async with</code> for automatic cleanup</li>
<li><strong>Health checks</strong>: Monitor database connection count</li>
<li><strong>Graceful shutdown</strong>: Close connections on service shutdown</li>
<li><strong>Connection timeout</strong>: Set reasonable timeout values</li>
<li><strong>Monitoring</strong>: Alert on high connection count</li>
</ol>
<hr />
<h2 id="memory-leaks"><a class="header" href="#memory-leaks">Memory Leaks</a></h2>
<h3 id="symptoms-3"><a class="header" href="#symptoms-3">Symptoms</a></h3>
<ul>
<li>Gradual memory increase over time</li>
<li>OOMKilled pod restarts (Kubernetes)</li>
<li>Swap usage increasing</li>
<li>Alert: <code>HighMemoryUsage</code></li>
</ul>
<h3 id="diagnosis-3"><a class="header" href="#diagnosis-3">Diagnosis</a></h3>
<p><strong>Step 1: Identify leaking service</strong></p>
<pre><code class="language-bash"># Monitor memory over time
docker stats

# Kubernetes
kubectl top pods -n octollm --watch

# Check for OOMKilled containers
kubectl get pods -n octollm -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.containerStatuses[0].lastState.terminated.reason}{"\n"}{end}'
</code></pre>
<p><strong>Step 2: Profile memory usage</strong></p>
<pre><code class="language-python"># Add memory profiling to orchestrator
# Install: pip install memory-profiler

from memory_profiler import profile

@profile
async def process_task(task_id: str):
    # Function code
    pass

# Run with:
# python -m memory_profiler app/main.py
</code></pre>
<pre><code class="language-python"># Track object counts
import gc
import sys

def get_memory_usage():
    """Get current memory usage details"""
    gc.collect()

    object_counts = {}
    for obj in gc.get_objects():
        obj_type = type(obj).__name__
        object_counts[obj_type] = object_counts.get(obj_type, 0) + 1

    # Sort by count
    sorted_counts = sorted(object_counts.items(), key=lambda x: x[1], reverse=True)

    return sorted_counts[:20]  # Top 20 object types
</code></pre>
<p><strong>Step 3: Check for common leak patterns</strong></p>
<pre><code class="language-python"># 1. Unclosed connections
# BAD:
client = httpx.AsyncClient()
await client.get("http://example.com")
# client never closed!

# GOOD:
async with httpx.AsyncClient() as client:
    await client.get("http://example.com")

# 2. Growing caches
# BAD:
cache = {}  # Unbounded cache
cache[key] = value  # Grows forever

# GOOD:
from cachetools import TTLCache
cache = TTLCache(maxsize=1000, ttl=3600)

# 3. Event listener leaks
# BAD:
emitter.on("event", handler)  # Handler never removed

# GOOD:
emitter.on("event", handler)
# ... later:
emitter.off("event", handler)
</code></pre>
<h3 id="resolution-3"><a class="header" href="#resolution-3">Resolution</a></h3>
<p><strong>Scenario A: Unbounded cache</strong></p>
<pre><code class="language-python"># Replace unbounded cache with TTL cache

# Before:
result_cache = {}  # Grows indefinitely

# After:
from cachetools import TTLCache

result_cache = TTLCache(
    maxsize=10000,      # Max 10k items
    ttl=3600            # 1 hour TTL
)

# Or use Redis with expiration
await redis_client.setex(key, 3600, value)
</code></pre>
<p><strong>Scenario B: Connection leaks</strong></p>
<pre><code class="language-python"># Audit all HTTP clients and database connections

# Create reusable client
from fastapi import FastAPI
import httpx

app = FastAPI()

@app.on_event("startup")
async def startup():
    app.state.http_client = httpx.AsyncClient(
        timeout=10.0,
        limits=httpx.Limits(max_keepalive_connections=20)
    )

@app.on_event("shutdown")
async def shutdown():
    await app.state.http_client.aclose()

# Use shared client
async def call_arm(request):
    client = app.state.http_client
    response = await client.post("http://arm/execute", json=request)
    return response
</code></pre>
<p><strong>Scenario C: Large object retention</strong></p>
<pre><code class="language-python"># Clear large objects after use

async def process_large_dataset(data):
    # Process data
    result = expensive_operation(data)

    # Explicitly clear references
    del data
    gc.collect()

    return result

# Use generators for large sequences
def iterate_tasks():
    # BAD: Load all tasks into memory
    tasks = Task.query.all()  # Could be millions
    for task in tasks:
        yield process(task)

    # GOOD: Use pagination
    page = 0
    while True:
        tasks = Task.query.limit(100).offset(page * 100).all()
        if not tasks:
            break
        for task in tasks:
            yield process(task)
        page += 1
</code></pre>
<p><strong>Scenario D: Circular references</strong></p>
<pre><code class="language-python"># Break circular references

# Problematic:
class Task:
    def __init__(self):
        self.subtasks = []

class SubTask:
    def __init__(self, parent):
        self.parent = parent  # Circular reference
        parent.subtasks.append(self)

# Fix with weak references:
import weakref

class SubTask:
    def __init__(self, parent):
        self.parent = weakref.ref(parent)  # Weak reference
        parent.subtasks.append(self)

    def get_parent(self):
        return self.parent()  # De-reference
</code></pre>
<h3 id="prevention-3"><a class="header" href="#prevention-3">Prevention</a></h3>
<ol>
<li><strong>Use context managers</strong>: For all resources (files, connections, clients)</li>
<li><strong>Bounded caches</strong>: Use TTLCache or LRU with size limits</li>
<li><strong>Weak references</strong>: For parent-child relationships</li>
<li><strong>Regular profiling</strong>: Run memory profiler in staging</li>
<li><strong>Resource limits</strong>: Set memory limits to catch leaks early</li>
<li><strong>Monitoring</strong>: Track memory usage over time</li>
</ol>
<hr />
<h2 id="task-routing-failures"><a class="header" href="#task-routing-failures">Task Routing Failures</a></h2>
<h3 id="symptoms-4"><a class="header" href="#symptoms-4">Symptoms</a></h3>
<ul>
<li>Tasks stuck in "pending" state</li>
<li>No appropriate arm found for task</li>
<li>Routing scores all zero</li>
<li>Tasks timing out</li>
</ul>
<h3 id="diagnosis-4"><a class="header" href="#diagnosis-4">Diagnosis</a></h3>
<p><strong>Step 1: Check task details</strong></p>
<pre><code class="language-bash"># View task in database
docker compose exec postgres psql -U octollm -c "
SELECT task_id, goal, status, created_at, updated_at
FROM tasks
WHERE task_id = 'task-123';"

# Check task routing history
docker compose exec postgres psql -U octollm -c "
SELECT * FROM action_log
WHERE task_id = 'task-123'
ORDER BY timestamp DESC;"
</code></pre>
<p><strong>Step 2: Verify arm availability</strong></p>
<pre><code class="language-bash"># Check arm health
for port in 8100 8101 8102 8103 8104 8105; do
  echo -n "Port $port: "
  curl -sf http://localhost:$port/health &amp;&amp; echo "✓" || echo "✗"
done

# Check arm capabilities
curl http://localhost:8100/capabilities | jq
</code></pre>
<p><strong>Step 3: Check orchestrator routing logic</strong></p>
<pre><code class="language-bash"># Enable debug logging
# In .env:
LOG_LEVEL=debug

docker compose restart orchestrator

# View routing decisions
docker compose logs -f orchestrator | grep -i "routing"
</code></pre>
<p><strong>Step 4: Test routing manually</strong></p>
<pre><code class="language-python"># In orchestrator container
docker compose exec orchestrator python

from app.services.router import ArmRouter
from app.models.task import TaskContract

router = ArmRouter()
task = TaskContract(
    goal="Write a Python function",
    constraints=[],
    priority="medium"
)

scores = await router.score_arms(task)
print(scores)
</code></pre>
<h3 id="resolution-4"><a class="header" href="#resolution-4">Resolution</a></h3>
<p><strong>Scenario A: All arms down</strong></p>
<pre><code class="language-bash"># Restart arms
docker compose restart planner-arm executor-arm coder-arm judge-arm guardian-arm retriever-arm

# Kubernetes
kubectl rollout restart deployment -l app-type=arm -n octollm
</code></pre>
<p><strong>Scenario B: Routing scoring issues</strong></p>
<pre><code class="language-python"># Fix routing algorithm in orchestrator/app/services/router.py

async def score_arms(self, task: TaskContract) -&gt; Dict[str, float]:
    """Score arms based on task requirements"""
    scores = {}

    for arm_name, arm_capability in self.registered_arms.items():
        score = 0.0

        # Check keyword matching
        task_keywords = extract_keywords(task.goal.lower())
        arm_keywords = arm_capability.keywords

        keyword_matches = len(set(task_keywords) &amp; set(arm_keywords))
        score += keyword_matches * 10

        # Check domain match
        if arm_capability.domain in task.goal.lower():
            score += 50

        # Penalize if arm is unhealthy
        if not await self.is_arm_healthy(arm_name):
            score = 0

        scores[arm_name] = score

    # If no scores, default to planner
    if all(s == 0 for s in scores.values()):
        scores["planner"] = 100

    return scores
</code></pre>
<p><strong>Scenario C: Capabilities not registered</strong></p>
<pre><code class="language-python"># Ensure arms register capabilities on startup
# In each arm's app/main.py

@app.on_event("startup")
async def register_with_orchestrator():
    """Register arm capabilities with orchestrator"""
    capability = ArmCapability(
        name="planner-arm",
        domain="planning",
        keywords=["plan", "decompose", "break down", "steps"],
        url=f"http://{os.getenv('HOSTNAME')}:8100"
    )

    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://orchestrator:8000/api/v1/arms/register",
            json=capability.dict()
        )

        if response.status_code != 200:
            logger.error("Failed to register with orchestrator", error=response.text)
        else:
            logger.info("Successfully registered with orchestrator")
</code></pre>
<p><strong>Scenario D: Task constraints too strict</strong></p>
<pre><code class="language-python"># Relax constraints if no match found
async def route_task(self, task: TaskContract) -&gt; str:
    """Route task to best arm"""
    scores = await self.score_arms(task)

    max_score_arm = max(scores, key=scores.get)
    max_score = scores[max_score_arm]

    # If no good match, try relaxing constraints
    if max_score &lt; 10:
        logger.warning(
            "No good arm match, relaxing constraints",
            task_id=task.task_id,
            original_goal=task.goal
        )

        # Remove optional constraints
        task.constraints = [c for c in task.constraints if "must" in c.lower()]

        # Re-score
        scores = await self.score_arms(task)
        max_score_arm = max(scores, key=scores.get)

    return max_score_arm
</code></pre>
<h3 id="prevention-4"><a class="header" href="#prevention-4">Prevention</a></h3>
<ol>
<li><strong>Health checks</strong>: Ensure all arms have health endpoints</li>
<li><strong>Registration</strong>: Auto-register arms on startup</li>
<li><strong>Fallback routing</strong>: Always have a default arm (planner)</li>
<li><strong>Monitoring</strong>: Track routing failures</li>
<li><strong>Testing</strong>: Test routing logic with various task types</li>
</ol>
<hr />
<h2 id="llm-api-failures"><a class="header" href="#llm-api-failures">LLM API Failures</a></h2>
<h3 id="symptoms-5"><a class="header" href="#symptoms-5">Symptoms</a></h3>
<ul>
<li>429 Too Many Requests errors</li>
<li>503 Service Unavailable from LLM provider</li>
<li>Authentication errors</li>
<li>Timeout errors</li>
<li>Alert: <code>HighLLMAPIErrorRate</code></li>
</ul>
<h3 id="diagnosis-5"><a class="header" href="#diagnosis-5">Diagnosis</a></h3>
<p><strong>Step 1: Check LLM API metrics</strong></p>
<pre><code class="language-bash"># Query Prometheus
curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=rate(llm_api_calls_total{status="error"}[5m])'

# Check error logs
docker compose logs orchestrator | grep -i "llm.*error"
</code></pre>
<p><strong>Step 2: Verify API key</strong></p>
<pre><code class="language-bash"># Test API key manually
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"

# Check key in environment
docker compose exec orchestrator env | grep OPENAI_API_KEY
</code></pre>
<p><strong>Step 3: Check rate limiting</strong></p>
<pre><code class="language-bash"># View rate limit headers from last request
docker compose logs orchestrator | grep -i "rate.*limit"

# Check current request rate
curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=rate(llm_api_calls_total[1m]) * 60'
</code></pre>
<h3 id="resolution-5"><a class="header" href="#resolution-5">Resolution</a></h3>
<p><strong>Scenario A: Rate limiting (429 errors)</strong></p>
<pre><code class="language-python"># Implement exponential backoff with jitter
import asyncio
import random
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

@retry(
    retry=retry_if_exception_type(httpx.HTTPStatusError),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    stop=stop_after_attempt(5)
)
async def call_llm_api(prompt: str) -&gt; str:
    """Call LLM API with exponential backoff"""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
            json={
                "model": "gpt-4",
                "messages": [{"role": "user", "content": prompt}]
            },
            timeout=60.0
        )

        if response.status_code == 429:
            # Add jitter to prevent thundering herd
            await asyncio.sleep(random.uniform(0, 2))
            response.raise_for_status()

        return response.json()
</code></pre>
<pre><code class="language-python"># Implement request queuing
from asyncio import Queue, Semaphore

class LLMClient:
    def __init__(self, max_concurrent=5, max_per_minute=50):
        self.semaphore = Semaphore(max_concurrent)
        self.rate_limiter = TokenBucket(max_per_minute, 60)

    async def generate(self, prompt: str) -&gt; str:
        async with self.semaphore:  # Limit concurrent requests
            await self.rate_limiter.acquire()  # Rate limit
            return await self._call_api(prompt)
</code></pre>
<p><strong>Scenario B: Service unavailable (503 errors)</strong></p>
<pre><code class="language-python"># Implement circuit breaker pattern
from circuitbreaker import circuit

@circuit(failure_threshold=5, recovery_timeout=60)
async def call_llm_with_circuit_breaker(prompt: str) -&gt; str:
    """Call LLM API with circuit breaker"""
    try:
        return await call_llm_api(prompt)
    except Exception as e:
        logger.error("LLM API call failed", error=str(e))
        raise

# Circuit opens after 5 failures, waits 60s before retry
</code></pre>
<pre><code class="language-python"># Implement fallback to alternative provider
async def generate_with_fallback(prompt: str) -&gt; str:
    """Try primary provider, fallback to secondary"""
    try:
        return await openai_client.generate(prompt)
    except Exception as e:
        logger.warning(
            "OpenAI failed, falling back to Anthropic",
            error=str(e)
        )
        return await anthropic_client.generate(prompt)
</code></pre>
<p><strong>Scenario C: Timeout errors</strong></p>
<pre><code class="language-python"># Increase timeout for long-running requests
client = httpx.AsyncClient(
    timeout=httpx.Timeout(
        connect=5.0,
        read=120.0,  # 2 minutes for completion
        write=5.0,
        pool=5.0
    )
)

# Stream responses for long generations
async def stream_llm_response(prompt: str):
    """Stream LLM response chunks"""
    async with client.stream(
        "POST",
        "https://api.openai.com/v1/chat/completions",
        json={
            "model": "gpt-4",
            "messages": [{"role": "user", "content": prompt}],
            "stream": True
        }
    ) as response:
        async for chunk in response.aiter_bytes():
            yield chunk
</code></pre>
<p><strong>Scenario D: Authentication errors</strong></p>
<pre><code class="language-bash"># Rotate API key
# Update .env file
OPENAI_API_KEY=sk-new-key-here

# Reload configuration
docker compose up -d orchestrator

# Kubernetes: Update secret
kubectl create secret generic octollm-secrets \
  --from-literal=OPENAI_API_KEY=sk-new-key \
  --dry-run=client -o yaml | kubectl apply -f -

kubectl rollout restart deployment orchestrator -n octollm
</code></pre>
<h3 id="prevention-5"><a class="header" href="#prevention-5">Prevention</a></h3>
<ol>
<li><strong>Rate limiting</strong>: Implement token bucket or leaky bucket</li>
<li><strong>Circuit breakers</strong>: Prevent cascading failures</li>
<li><strong>Retries</strong>: Use exponential backoff with jitter</li>
<li><strong>Fallback providers</strong>: Have secondary LLM provider</li>
<li><strong>Caching</strong>: Cache LLM responses when possible</li>
<li><strong>Monitoring</strong>: Track API error rates and costs</li>
</ol>
<hr />
<h2 id="cache-performance-issues"><a class="header" href="#cache-performance-issues">Cache Performance Issues</a></h2>
<h3 id="symptoms-6"><a class="header" href="#symptoms-6">Symptoms</a></h3>
<ul>
<li>Low cache hit rate (&lt;50%)</li>
<li>Redis memory full</li>
<li>Slow cache lookups</li>
<li>Alert: <code>CacheMissRate</code></li>
</ul>
<h3 id="diagnosis-6"><a class="header" href="#diagnosis-6">Diagnosis</a></h3>
<p><strong>Step 1: Check cache hit rate</strong></p>
<pre><code class="language-bash"># Query Prometheus
curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))'
</code></pre>
<p><strong>Step 2: Check Redis stats</strong></p>
<pre><code class="language-bash"># Redis info
docker compose exec redis redis-cli INFO stats

# Check memory usage
docker compose exec redis redis-cli INFO memory

# Check key count
docker compose exec redis redis-cli DBSIZE

# Sample keys
docker compose exec redis redis-cli --scan --pattern "*" | head -20
</code></pre>
<p><strong>Step 3: Analyze cache usage patterns</strong></p>
<pre><code class="language-bash"># Monitor cache commands
docker compose exec redis redis-cli MONITOR

# Check slow queries
docker compose exec redis redis-cli SLOWLOG GET 10
</code></pre>
<h3 id="resolution-6"><a class="header" href="#resolution-6">Resolution</a></h3>
<p><strong>Scenario A: Cache eviction policy issues</strong></p>
<pre><code class="language-bash"># Check current policy
docker compose exec redis redis-cli CONFIG GET maxmemory-policy

# Set appropriate policy for use case
docker compose exec redis redis-cli CONFIG SET maxmemory-policy allkeys-lru

# Options:
# - allkeys-lru: Evict any key, LRU
# - volatile-lru: Evict keys with TTL, LRU
# - allkeys-lfu: Evict any key, LFU (least frequently used)
# - volatile-ttl: Evict keys with shortest TTL
</code></pre>
<p><strong>Scenario B: Inefficient cache keys</strong></p>
<pre><code class="language-python"># Bad: Too specific keys (low hit rate)
cache_key = f"task:{task_id}:{user_id}:{timestamp}"

# Good: Normalized keys
cache_key = f"task:{task_id}"

# Bad: Large values cached
await redis.set("large_dataset", json.dumps(huge_object))  # MB of data

# Good: Cache references or summaries
await redis.set(f"dataset:{id}:summary", summary)  # Small summary
# Store full data in database
</code></pre>
<p><strong>Scenario C: Missing cache warming</strong></p>
<pre><code class="language-python"># Implement cache warming on startup
@app.on_event("startup")
async def warm_cache():
    """Pre-populate cache with frequently accessed data"""
    logger.info("Warming cache...")

    # Load arm capabilities
    arms = await db.query("SELECT * FROM arms WHERE enabled = true")
    for arm in arms:
        await redis.setex(
            f"arm:capability:{arm.name}",
            3600,
            json.dumps(arm.capabilities)
        )

    # Load common entity relationships
    entities = await db.query(
        "SELECT * FROM entities WHERE access_count &gt; 100"
    )
    for entity in entities:
        await redis.setex(
            f"entity:{entity.id}",
            3600,
            json.dumps(entity.dict())
        )

    logger.info(f"Cache warmed with {len(arms) + len(entities)} entries")
</code></pre>
<p><strong>Scenario D: Cache stampede</strong></p>
<pre><code class="language-python"># Prevent cache stampede with locking
import asyncio
from contextlib import asynccontextmanager

class CacheWithLock:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.locks = {}

    @asynccontextmanager
    async def lock(self, key: str):
        """Acquire lock for cache key"""
        lock_key = f"lock:{key}"
        lock_id = str(uuid.uuid4())

        # Try to acquire lock
        while not await self.redis.set(lock_key, lock_id, nx=True, ex=10):
            await asyncio.sleep(0.1)  # Wait for lock

        try:
            yield
        finally:
            # Release lock
            if await self.redis.get(lock_key) == lock_id:
                await self.redis.delete(lock_key)

    async def get_or_compute(self, key: str, compute_fn):
        """Get from cache or compute with lock"""
        # Try cache first
        cached = await self.redis.get(key)
        if cached:
            return json.loads(cached)

        # Cache miss - acquire lock to prevent stampede
        async with self.lock(key):
            # Double-check cache (another thread may have computed)
            cached = await self.redis.get(key)
            if cached:
                return json.loads(cached)

            # Compute value
            value = await compute_fn()

            # Cache result
            await self.redis.setex(key, 3600, json.dumps(value))

            return value
</code></pre>
<h3 id="prevention-6"><a class="header" href="#prevention-6">Prevention</a></h3>
<ol>
<li><strong>Appropriate TTLs</strong>: Set expiration based on data change frequency</li>
<li><strong>Cache warming</strong>: Pre-populate cache on startup</li>
<li><strong>Consistent keys</strong>: Use normalized cache keys</li>
<li><strong>Monitoring</strong>: Track hit rate and memory usage</li>
<li><strong>Eviction policy</strong>: Choose policy matching access patterns</li>
</ol>
<hr />
<h2 id="resource-exhaustion"><a class="header" href="#resource-exhaustion">Resource Exhaustion</a></h2>
<h3 id="symptoms-7"><a class="header" href="#symptoms-7">Symptoms</a></h3>
<ul>
<li>CPU at 100%</li>
<li>Memory at limit</li>
<li>Disk space full</li>
<li>Alert: <code>HighCPUUsage</code>, <code>HighMemoryUsage</code>, <code>DiskSpaceLow</code></li>
</ul>
<h3 id="diagnosis-7"><a class="header" href="#diagnosis-7">Diagnosis</a></h3>
<pre><code class="language-bash"># Check resource usage
docker stats

# Kubernetes
kubectl top pods -n octollm
kubectl top nodes

# Check disk usage
df -h
docker system df

# Identify resource-heavy processes
docker compose exec orchestrator top
</code></pre>
<h3 id="resolution-7"><a class="header" href="#resolution-7">Resolution</a></h3>
<p><strong>CPU exhaustion:</strong></p>
<pre><code class="language-bash"># Identify CPU-heavy services
docker stats --no-stream | sort -k3 -hr

# Scale horizontally
kubectl scale deployment orchestrator --replicas=3 -n octollm

# Optimize code (add CPU profiling)
python -m cProfile app/main.py
</code></pre>
<p><strong>Memory exhaustion:</strong></p>
<pre><code class="language-bash"># Clear caches
docker compose exec redis redis-cli FLUSHDB

# Restart services
docker compose restart

# Increase limits
kubectl edit deployment orchestrator -n octollm
</code></pre>
<p><strong>Disk exhaustion:</strong></p>
<pre><code class="language-bash"># Clean up Docker
docker system prune -a --volumes

# Rotate logs
docker compose logs --no-log-prefix &gt; /dev/null

# Clean old backups
find /backups -mtime +30 -delete
</code></pre>
<h3 id="prevention-7"><a class="header" href="#prevention-7">Prevention</a></h3>
<ol>
<li><strong>Resource limits</strong>: Set CPU/memory limits</li>
<li><strong>Auto-scaling</strong>: Configure HPA in Kubernetes</li>
<li><strong>Monitoring</strong>: Alert on resource usage</li>
<li><strong>Log rotation</strong>: Limit log file sizes</li>
<li><strong>Regular cleanup</strong>: Schedule cleanup jobs</li>
</ol>
<hr />
<h2 id="security-violations"><a class="header" href="#security-violations">Security Violations</a></h2>
<h3 id="symptoms-8"><a class="header" href="#symptoms-8">Symptoms</a></h3>
<ul>
<li>Alert: <code>SecurityViolationDetected</code></li>
<li>PII detected in logs</li>
<li>Suspicious commands blocked</li>
<li>Unauthorized access attempts</li>
</ul>
<h3 id="diagnosis-8"><a class="header" href="#diagnosis-8">Diagnosis</a></h3>
<pre><code class="language-bash"># Check security logs
docker compose logs guardian-arm | grep -i "violation"

# Query security metrics
curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=security_violations_total'
</code></pre>
<h3 id="resolution-8"><a class="header" href="#resolution-8">Resolution</a></h3>
<pre><code class="language-bash"># Review and update security rules
# In guardian-arm configuration

# Block command execution
docker compose exec guardian-arm cat /app/config/blocked_commands.txt

# Review PII detection patterns
docker compose logs guardian-arm | grep "PII detected"

# Update firewall rules if needed
</code></pre>
<h3 id="prevention-8"><a class="header" href="#prevention-8">Prevention</a></h3>
<ol>
<li><strong>Input validation</strong>: Validate all user inputs</li>
<li><strong>PII detection</strong>: Scan all inputs/outputs</li>
<li><strong>Audit logging</strong>: Log all security events</li>
<li><strong>Regular audits</strong>: Review security logs</li>
<li><strong>Security training</strong>: Educate team on security</li>
</ol>
<hr />
<h2 id="data-corruption"><a class="header" href="#data-corruption">Data Corruption</a></h2>
<h3 id="symptoms-9"><a class="header" href="#symptoms-9">Symptoms</a></h3>
<ul>
<li>Invalid data in database</li>
<li>Foreign key violations</li>
<li>Inconsistent entity relationships</li>
<li>Application errors due to malformed data</li>
</ul>
<h3 id="diagnosis-9"><a class="header" href="#diagnosis-9">Diagnosis</a></h3>
<pre><code class="language-sql">-- Check for orphaned records
SELECT * FROM relationships r
LEFT JOIN entities e1 ON r.from_entity_id = e1.entity_id
WHERE e1.entity_id IS NULL;

-- Check for invalid JSON
SELECT * FROM entities
WHERE jsonb_typeof(properties) != 'object';

-- Check constraints
SELECT conname, pg_get_constraintdef(oid)
FROM pg_constraint
WHERE conrelid = 'tasks'::regclass;
</code></pre>
<h3 id="resolution-9"><a class="header" href="#resolution-9">Resolution</a></h3>
<pre><code class="language-sql">-- Fix orphaned relationships
DELETE FROM relationships
WHERE from_entity_id NOT IN (SELECT entity_id FROM entities)
   OR to_entity_id NOT IN (SELECT entity_id FROM entities);

-- Fix invalid JSON
UPDATE entities
SET properties = '{}'::jsonb
WHERE jsonb_typeof(properties) != 'object';

-- Restore from backup if needed
docker compose exec -T postgres psql -U octollm octollm &lt; backup.sql
</code></pre>
<h3 id="prevention-9"><a class="header" href="#prevention-9">Prevention</a></h3>
<ol>
<li><strong>Foreign keys</strong>: Use database constraints</li>
<li><strong>Validation</strong>: Validate data before insert</li>
<li><strong>Transactions</strong>: Use atomic operations</li>
<li><strong>Backups</strong>: Regular automated backups</li>
<li><strong>Testing</strong>: Test data integrity</li>
</ol>
<hr />
<h2 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h2>
<h3 id="common-commands"><a class="header" href="#common-commands">Common Commands</a></h3>
<pre><code class="language-bash"># Check service health
curl http://localhost:8000/health

# View logs
docker compose logs -f [service]

# Restart service
docker compose restart [service]

# Check resource usage
docker stats

# Access database
docker compose exec postgres psql -U octollm

# Access Redis
docker compose exec redis redis-cli

# Check metrics
curl http://localhost:9090/metrics
</code></pre>
<h3 id="emergency-procedures"><a class="header" href="#emergency-procedures">Emergency Procedures</a></h3>
<p><strong>Complete system restart:</strong></p>
<pre><code class="language-bash"># Stop all services
docker compose down

# Clear caches (optional)
docker compose down -v

# Start services
docker compose up -d

# Verify health
./scripts/healthcheck.sh
</code></pre>
<p><strong>Rollback deployment (Kubernetes):</strong></p>
<pre><code class="language-bash"># View rollout history
kubectl rollout history deployment orchestrator -n octollm

# Rollback to previous version
kubectl rollout undo deployment orchestrator -n octollm

# Rollback to specific revision
kubectl rollout undo deployment orchestrator --to-revision=3 -n octollm
</code></pre>
<hr />
<h2 id="escalation-procedures"><a class="header" href="#escalation-procedures">Escalation Procedures</a></h2>
<h3 id="level-1-on-call-engineer"><a class="header" href="#level-1-on-call-engineer">Level 1: On-call Engineer</a></h3>
<ul>
<li>Service unavailable</li>
<li>High latency</li>
<li>Database connection issues</li>
</ul>
<p><strong>Actions</strong>:</p>
<ol>
<li>Follow relevant playbook</li>
<li>Restart affected services</li>
<li>Escalate if unresolved in 15 minutes</li>
</ol>
<h3 id="level-2-senior-engineer"><a class="header" href="#level-2-senior-engineer">Level 2: Senior Engineer</a></h3>
<ul>
<li>Memory leaks</li>
<li>Resource exhaustion</li>
<li>Data corruption</li>
</ul>
<p><strong>Actions</strong>:</p>
<ol>
<li>Deep diagnosis with profiling</li>
<li>Code fixes if needed</li>
<li>Escalate to engineering lead if architectural issue</li>
</ol>
<h3 id="level-3-engineering-lead"><a class="header" href="#level-3-engineering-lead">Level 3: Engineering Lead</a></h3>
<ul>
<li>Security violations</li>
<li>Architectural issues</li>
<li>Multi-service failures</li>
</ul>
<p><strong>Actions</strong>:</p>
<ol>
<li>Coordinate team response</li>
<li>Make architectural decisions</li>
<li>Communicate with stakeholders</li>
</ol>
<hr />
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="monitoring-alerting.html">Monitoring and Alerting</a> - Set up observability</li>
<li><a href="performance-tuning.html">Performance Tuning</a> - Optimize performance</li>
<li><a href="kubernetes-deployment.html">Kubernetes Deployment</a> - Production deployment</li>
<li><a href="docker-compose-setup.html">Docker Compose Setup</a> - Local setup</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../operations/alert-response-procedures.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../operations/performance-tuning.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../operations/alert-response-procedures.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../operations/performance-tuning.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
