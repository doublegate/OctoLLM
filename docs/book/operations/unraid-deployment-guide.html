<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Unraid Deployment Guide - OctoLLM Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Distributed AI Architecture for Offensive Security and Developer Tooling - Comprehensive technical documentation covering architecture, API, development, operations, and security.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">OctoLLM Documentation</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/doublegate/OctoLLM" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/doublegate/OctoLLM/edit/main/docs/src/operations/unraid-deployment-guide.md" title="Suggest an edit" aria-label="Suggest an edit" rel="edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="octollm-unraid-deployment-guide"><a class="header" href="#octollm-unraid-deployment-guide">OctoLLM Unraid Deployment Guide</a></h1>
<p>Complete guide for deploying OctoLLM on Unraid 7.2.0 with Dell PowerEdge R730xd hardware.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#hardware-requirements">Hardware Requirements</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#gpu-setup">GPU Setup</a></li>
<li><a href="#managing-services">Managing Services</a></li>
<li><a href="#accessing-services">Accessing Services</a></li>
<li><a href="#local-llm-usage">Local LLM Usage</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
<li><a href="#backup--restore">Backup &amp; Restore</a></li>
<li><a href="#performance-tuning">Performance Tuning</a></li>
<li><a href="#monitoring">Monitoring</a></li>
<li><a href="#security">Security</a></li>
<li><a href="#migration-to-cloud">Migration to Cloud</a></li>
</ol>
<hr />
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>OctoLLM is a distributed AI architecture inspired by octopus neurobiology. This guide covers local deployment on Unraid, optimized for development with GPU-accelerated LLM inference.</p>
<h3 id="why-unraid"><a class="header" href="#why-unraid">Why Unraid?</a></h3>
<ul>
<li><strong>Native Docker Support</strong>: Excellent Docker management UI</li>
<li><strong>Hardware Flexibility</strong>: Mix and match drives, use cache effectively</li>
<li><strong>GPU Passthrough</strong>: Strong support for NVIDIA GPUs</li>
<li><strong>Community</strong>: Large community with extensive documentation</li>
</ul>
<h3 id="deployment-architecture"><a class="header" href="#deployment-architecture">Deployment Architecture</a></h3>
<pre><code>┌───────────────────────────────────────────────────────────┐
│                    Unraid Host (bond0)                    │
│  ┌─────────────────────────────────────────────────────┐  │
│  │         Docker Bridge: octollm-net (172.20.0.0/16)  │  │
│  │                                                     │  │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────────────┐   │  │
│  │  │  Reflex  │  │Orchestr. │  │  6 Arms          │   │  │
│  │  │  Layer   │  │          │  │  (Planner,       │   │  │
│  │  │  (Rust)  │  │ (Python) │  │   Executor,      │   │  │
│  │  │          │  │          │  │   Retriever,     │   │  │
│  │  │  :3001   │  │  :3000   │  │   Coder,         │   │  │
│  │  │          │  │          │  │   Judge,         │   │  │
│  │  │          │  │          │  │   Guardian)      │   │  │
│  │  │          │  │          │  │  :6001-6006      │   │  │
│  │  └────┬─────┘  └────┬─────┘  └────────┬─────────┘   │  │
│  │       │             │                 │             │  │
│  │       └─────────────┴─────────────────┘             │  │
│  │                     │                               │  │
│  │  ┌──────────────────┴──────────────────────┐        │  │
│  │  │                                         │        │  │
│  │  ▼                                         ▼        │  │
│  │  ┌──────────┐  ┌──────┐  ┌──────┐  ┌──────────┐     │  │
│  │  │PostgreSQL│  │Redis │  │Qdrant│  │  Ollama  │     │  │
│  │  │  15      │  │  7   │  │ 1.7.4│  │ (Models) │     │  │
│  │  │  :3010   │  │:3011 │  │:3012 │  │  :3014   │     │  │
│  │  └──────────┘  └──────┘  └──────┘  └──────┬───┘     │  │
│  │                                           │         │  │
│  │  ┌──────────────────────────────────────┐ │         │  │
│  │  │       Monitoring Stack               │ │         │  │
│  │  │  ┌──────────┐  ┌────────┐ ┌──────┐   │ │         │  │
│  │  │  │Prometheus│  │Grafana │ │ Loki │   │ │         │  │
│  │  │  │  :9090   │  │ :3030  │ │:3100 │   │ │         │  │
│  │  │  └──────────┘  └────────┘ └──────┘   │ │         │  │
│  │  └──────────────────────────────────────┘ │         │  │
│  └───────────────────────────────────────────┼─────────┘  │
│                                              │            │
│                                         ┌────▼──────┐     │
│                                         │ Tesla P40 │     │
│                                         │  24GB     │     │
│                                         │  VRAM     │     │
│                                         └───────────┘     │
└───────────────────────────────────────────────────────────┘
</code></pre>
<hr />
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<h3 id="software-requirements"><a class="header" href="#software-requirements">Software Requirements</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Software</th><th>Minimum Version</th><th>Recommended</th><th>Purpose</th></tr></thead><tbody>
<tr><td>Unraid</td><td>7.0.0</td><td>7.2.0+</td><td>Host OS</td></tr>
<tr><td>Docker</td><td>20.10</td><td>27.5.1+</td><td>Container runtime</td></tr>
<tr><td>Docker Compose</td><td>1.29</td><td>2.40.3+ (V2)</td><td>Orchestration</td></tr>
<tr><td>NVIDIA Driver</td><td>510+</td><td>580.105.08+</td><td>GPU support</td></tr>
</tbody></table>
</div>
<h3 id="unraid-plugins-required"><a class="header" href="#unraid-plugins-required">Unraid Plugins Required</a></h3>
<p>Install from Community Applications:</p>
<ol>
<li>
<p><strong>NVIDIA Driver</strong> (for GPU support)</p>
<ul>
<li>Search: "nvidia driver"</li>
<li>Install: "nvidia-driver" by ich777</li>
<li>Reboot after installation</li>
</ul>
</li>
<li>
<p><strong>Compose Manager</strong> (optional, for UI management)</p>
<ul>
<li>Search: "compose manager"</li>
<li>Install: "compose.manager" by dcflachs</li>
</ul>
</li>
<li>
<p><strong>NerdTools</strong> (optional, for additional utilities)</p>
<ul>
<li>Useful for jq, git, and other tools</li>
</ul>
</li>
</ol>
<h3 id="user-account-setup"><a class="header" href="#user-account-setup">User Account Setup</a></h3>
<p>Create Unraid user account with access to:</p>
<ul>
<li>Docker management</li>
<li>Console/SSH access</li>
<li>Appdata shares</li>
</ul>
<hr />
<h2 id="hardware-requirements"><a class="header" href="#hardware-requirements">Hardware Requirements</a></h2>
<h3 id="minimum-configuration"><a class="header" href="#minimum-configuration">Minimum Configuration</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Component</th><th>Minimum</th><th>Recommended</th><th>Notes</th></tr></thead><tbody>
<tr><td>CPU</td><td>4 cores</td><td>8+ cores</td><td>More cores = better parallelism</td></tr>
<tr><td>RAM</td><td>16GB</td><td>64GB+</td><td>More RAM = larger models</td></tr>
<tr><td>Storage</td><td>50GB free</td><td>200GB+ free</td><td>Models are large (5-50GB each)</td></tr>
<tr><td>GPU</td><td>None</td><td>NVIDIA Tesla P40</td><td>Optional but highly recommended</td></tr>
<tr><td>Network</td><td>100Mbps</td><td>1Gbps+</td><td>For model downloads</td></tr>
</tbody></table>
</div>
<h3 id="recommended-dell-poweredge-r730xd"><a class="header" href="#recommended-dell-poweredge-r730xd">Recommended: Dell PowerEdge R730xd</a></h3>
<p>This guide is optimized for:</p>
<pre><code>CPU:     Dual Intel Xeon E5-2683 v4 @ 2.10GHz
         - 32 physical cores (64 threads with HT)
         - 2 NUMA nodes
         - 40MB L3 cache

RAM:     503.8 GiB DDR4 ECC
         - 16× 32GB DIMMs
         - 2400 MHz
         - Error-correcting for reliability

GPU:     NVIDIA Tesla P40
         - 24GB GDDR5 VRAM
         - 3840 CUDA cores
         - 250W TDP
         - CUDA 13.0 support

Storage: 144TB array (10 disks)
         - 1.8TB SSD cache (btrfs)
         - 128GB Docker vDisk

Network: 4× Intel I350 Gigabit NICs
         - Bonded to 4Gbps aggregate (bond0)
         - LACP mode 4
</code></pre>
<h3 id="gpu-compatibility"><a class="header" href="#gpu-compatibility">GPU Compatibility</a></h3>
<p><strong>Supported GPUs</strong> (tested):</p>
<ul>
<li>NVIDIA Tesla P40 (24GB) ✅</li>
<li>NVIDIA Tesla P100 (16GB) ✅</li>
<li>NVIDIA Tesla V100 (32GB) ✅</li>
<li>NVIDIA RTX 3090 (24GB) ✅</li>
<li>NVIDIA RTX 4090 (24GB) ✅</li>
</ul>
<p><strong>Minimum VRAM for models</strong>:</p>
<ul>
<li>Small models (7-13B): 8GB VRAM</li>
<li>Medium models (30-70B): 24GB VRAM</li>
<li>Large models (70B+): 48GB+ VRAM or multi-GPU</li>
</ul>
<hr />
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<h3 id="step-1-install-nvidia-driver-plugin"><a class="header" href="#step-1-install-nvidia-driver-plugin">Step 1: Install NVIDIA Driver Plugin</a></h3>
<ol>
<li>Open Unraid WebUI: http://tower.local (or your server IP)</li>
<li>Navigate to <strong>Apps</strong> tab</li>
<li>Search for "nvidia driver"</li>
<li>Click <strong>Install</strong> on "nvidia-driver" by ich777</li>
<li>Wait for installation to complete</li>
<li><strong>Reboot server</strong></li>
<li>After reboot, verify:</li>
</ol>
<pre><code class="language-bash"># SSH to Unraid
ssh root@tower.local

# Test NVIDIA driver
nvidia-smi
</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 580.105.08     Driver Version: 580.105.08   CUDA Version: 13.0 |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P40           Off  | 00000000:03:00.0 Off |                    0 |
| N/A   30C    P0    49W / 250W |      0MiB / 24576MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre>
<h3 id="step-2-clone-repository"><a class="header" href="#step-2-clone-repository">Step 2: Clone Repository</a></h3>
<pre><code class="language-bash"># SSH to Unraid
ssh root@tower.local

# Navigate to appdata
cd /mnt/user/appdata

# Clone OctoLLM repository
git clone https://github.com/your-org/octollm.git
cd octollm
</code></pre>
<h3 id="step-3-run-setup-script"><a class="header" href="#step-3-run-setup-script">Step 3: Run Setup Script</a></h3>
<p>The automated setup script will:</p>
<ul>
<li>Create directory structure</li>
<li>Generate secure passwords</li>
<li>Configure environment files</li>
<li>Download Ollama models</li>
<li>Initialize databases</li>
<li>Start all services</li>
</ul>
<pre><code class="language-bash">cd /mnt/user/appdata/octollm/infrastructure/unraid

# Make script executable (if needed)
chmod +x setup-unraid.sh

# Run setup
bash setup-unraid.sh
</code></pre>
<p><strong>Setup Process</strong>:</p>
<pre><code>[INFO] Checking prerequisites...
[SUCCESS] Docker is installed: Docker version 27.5.1
[SUCCESS] Docker Compose V2 is installed: 2.40.3
[SUCCESS] NVIDIA driver is installed: 580.105.08
[SUCCESS] Detected GPU: Tesla P40 with 24576 MiB VRAM

[INFO] Creating directory structure in /mnt/user/appdata/octollm/...
[SUCCESS] Created directory: /mnt/user/appdata/octollm/postgres/data
[SUCCESS] Created directory: /mnt/user/appdata/octollm/redis/data
...

[INFO] Setting up environment configuration...
[SUCCESS] Environment file created: .env.unraid
[INFO] Secure passwords generated. Save these credentials:
PostgreSQL Password: xK9fL2mN8vP4qR7sT1wU6yZ3aB5cD0eF
Redis Password: gH4jK1lM7nP9qR2sT8vW5xY0zA3bC6dE
Qdrant API Key: fG1hI4jK7lM0nP3qR6sT9uV2wX5yZ8aB
Grafana Admin Password: cD0eF3gH6iJ9kL2mN5oP8qR1sT4uV7wX

[INFO] Creating PostgreSQL initialization script...
[SUCCESS] PostgreSQL initialization script created

[INFO] Setting up GPU and downloading Ollama models...
[WARNING] This may take 15-30 minutes depending on your internet speed.
[INFO] Pulling model: llama3.1:8b
[SUCCESS] Model llama3.1:8b downloaded successfully
...

[INFO] Starting OctoLLM services...
[SUCCESS] OctoLLM services started successfully

============================================================================
[SUCCESS] OctoLLM Unraid Setup Complete!
============================================================================

Access URLs:
  Orchestrator API:    http://192.168.4.6:3000
  Orchestrator Docs:   http://192.168.4.6:3000/docs
  Reflex Layer API:    http://192.168.4.6:3001
  Grafana Dashboard:   http://192.168.4.6:3030
  Prometheus:          http://192.168.4.6:9090
  Ollama API:          http://192.168.4.6:3014

Credentials:
  Grafana:
    Username: admin
    Password: cD0eF3gH6iJ9kL2mN5oP8qR1sT4uV7wX
</code></pre>
<h3 id="step-4-verify-installation"><a class="header" href="#step-4-verify-installation">Step 4: Verify Installation</a></h3>
<p>Run test suite:</p>
<pre><code class="language-bash"># Test prerequisites
bash tests/test-prerequisites.sh

# Test GPU access
bash tests/test-gpu.sh

# Test Ollama inference
bash tests/test-ollama.sh

# Test service health (wait 2-3 minutes after startup)
bash tests/test-services.sh
</code></pre>
<p><strong>All tests should pass</strong>:</p>
<pre><code>============================================================================
OctoLLM Service Health Test
============================================================================

[PASS] orchestrator is healthy
[PASS] reflex-layer is healthy
[PASS] planner-arm is healthy
...

============================================================================
Summary: 11 passed, 0 failed
============================================================================
[SUCCESS] All services are healthy!
</code></pre>
<hr />
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<h3 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h3>
<p>Edit <code>/mnt/user/appdata/octollm/infrastructure/unraid/.env.unraid</code>:</p>
<pre><code class="language-bash"># Network Configuration
HOST_IP=192.168.4.6                    # Change to your Unraid server IP

# Database Credentials (auto-generated by setup)
POSTGRES_DB=octollm
POSTGRES_USER=octollm
POSTGRES_PASSWORD=xK9fL2mN8vP4qR7sT1wU6yZ3aB5cD0eF
REDIS_PASSWORD=gH4jK1lM7nP9qR2sT8vW5xY0zA3bC6dE
QDRANT_API_KEY=fG1hI4jK7lM0nP3qR6sT9uV2wX5yZ8aB

# Local LLM Configuration
PREFER_LOCAL_LLM=true                  # Use GPU-accelerated local inference
OLLAMA_PRIMARY_MODEL=llama3.1:8b       # Fast general-purpose model
OLLAMA_FALLBACK_MODEL=mixtral:8x7b     # Advanced reasoning model
OLLAMA_NUM_PARALLEL=4                  # Concurrent requests (GPU memory limited)

# Cloud LLM APIs (optional fallback)
OPENAI_API_KEY=                        # Leave empty to skip
ANTHROPIC_API_KEY=                     # Leave empty to skip

# Performance Tuning
MAX_PARALLEL_ARMS=5                    # Max concurrent arm executions
TASK_TIMEOUT=300                       # Task timeout in seconds
CACHE_TTL=3600                         # Cache time-to-live in seconds

# Monitoring
LOG_LEVEL=INFO                         # DEBUG, INFO, WARNING, ERROR
GRAFANA_ADMIN_PASSWORD=cD0eF3gH6iJ9kL2mN5oP8qR1sT4uV7wX
</code></pre>
<h3 id="port-customization"><a class="header" href="#port-customization">Port Customization</a></h3>
<p>If ports conflict with existing services, edit <code>docker-compose.unraid.yml</code>:</p>
<pre><code class="language-yaml">services:
  orchestrator:
    ports:
      - "8000:8000"  # Change 3000 → 8000 if needed

  grafana:
    ports:
      - "3050:3000"  # Change 3030 → 3050 if needed
</code></pre>
<p><strong>After changes, restart services</strong>:</p>
<pre><code class="language-bash">docker-compose down
docker-compose up -d
</code></pre>
<hr />
<h2 id="gpu-setup"><a class="header" href="#gpu-setup">GPU Setup</a></h2>
<h3 id="installing-nvidia-driver"><a class="header" href="#installing-nvidia-driver">Installing NVIDIA Driver</a></h3>
<p><strong>Method 1: Unraid Plugin (Recommended)</strong></p>
<ol>
<li>Apps → Search "nvidia driver"</li>
<li>Install "nvidia-driver" by ich777</li>
<li>Reboot</li>
<li>Verify: <code>nvidia-smi</code></li>
</ol>
<p><strong>Method 2: Manual Installation</strong></p>
<pre><code class="language-bash"># Download driver
cd /tmp
wget https://us.download.nvidia.com/XFree86/Linux-x86_64/580.105.08/NVIDIA-Linux-x86_64-580.105.08.run

# Install
chmod +x NVIDIA-Linux-x86_64-580.105.08.run
./NVIDIA-Linux-x86_64-580.105.08.run --no-questions --ui=none

# Reboot
reboot
</code></pre>
<h3 id="configuring-docker-nvidia-runtime"><a class="header" href="#configuring-docker-nvidia-runtime">Configuring Docker NVIDIA Runtime</a></h3>
<p>Edit <code>/etc/docker/daemon.json</code>:</p>
<pre><code class="language-json">{
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  },
  "default-runtime": "nvidia"
}
</code></pre>
<p>Restart Docker:</p>
<pre><code class="language-bash">/etc/rc.d/rc.docker restart
</code></pre>
<h3 id="testing-gpu-access"><a class="header" href="#testing-gpu-access">Testing GPU Access</a></h3>
<pre><code class="language-bash"># Test from host
nvidia-smi

# Test from Docker
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
</code></pre>
<h3 id="gpu-monitoring"><a class="header" href="#gpu-monitoring">GPU Monitoring</a></h3>
<p><strong>Real-time monitoring</strong>:</p>
<pre><code class="language-bash"># Simple watch
nvidia-smi -l 1

# Detailed with scripts/monitor-resources.sh
cd /mnt/user/appdata/octollm/infrastructure/unraid
bash scripts/monitor-resources.sh
</code></pre>
<p><strong>Grafana dashboard</strong>:</p>
<ul>
<li>Navigate to http://192.168.4.6:3030</li>
<li>Login with admin / [password from .env.unraid]</li>
<li>Dashboard: "OctoLLM Unraid Dashboard"</li>
<li>GPU section shows:
<ul>
<li>Utilization %</li>
<li>Temperature</li>
<li>Memory usage</li>
<li>Power consumption</li>
</ul>
</li>
</ul>
<hr />
<h2 id="managing-services"><a class="header" href="#managing-services">Managing Services</a></h2>
<h3 id="docker-compose-commands"><a class="header" href="#docker-compose-commands">Docker Compose Commands</a></h3>
<p><strong>Navigate to compose directory first</strong>:</p>
<pre><code class="language-bash">cd /mnt/user/appdata/octollm/infrastructure/unraid
</code></pre>
<p><strong>Start all services</strong>:</p>
<pre><code class="language-bash">docker-compose up -d
</code></pre>
<p><strong>Stop all services</strong>:</p>
<pre><code class="language-bash">docker-compose stop
</code></pre>
<p><strong>Restart all services</strong>:</p>
<pre><code class="language-bash">docker-compose restart
</code></pre>
<p><strong>Stop and remove containers</strong>:</p>
<pre><code class="language-bash">docker-compose down
</code></pre>
<p><strong>View status</strong>:</p>
<pre><code class="language-bash">docker-compose ps
</code></pre>
<p><strong>View logs</strong>:</p>
<pre><code class="language-bash"># All services
docker-compose logs -f

# Specific service
docker-compose logs -f orchestrator

# Last 100 lines
docker-compose logs --tail=100 orchestrator
</code></pre>
<h3 id="individual-service-management"><a class="header" href="#individual-service-management">Individual Service Management</a></h3>
<p><strong>Restart single service</strong>:</p>
<pre><code class="language-bash">docker-compose restart orchestrator
</code></pre>
<p><strong>Rebuild single service</strong>:</p>
<pre><code class="language-bash">docker-compose build orchestrator
docker-compose up -d orchestrator
</code></pre>
<p><strong>Scale arms</strong> (if needed):</p>
<pre><code class="language-bash">docker-compose up -d --scale planner-arm=2
</code></pre>
<h3 id="unraid-docker-ui"><a class="header" href="#unraid-docker-ui">Unraid Docker UI</a></h3>
<p>Services also appear in Unraid Docker tab:</p>
<ul>
<li>Click container name to view logs</li>
<li>Click "Console" for shell access</li>
<li>Click "Edit" to modify settings</li>
<li>Use "Autostart" to start on boot</li>
</ul>
<hr />
<h2 id="accessing-services"><a class="header" href="#accessing-services">Accessing Services</a></h2>
<h3 id="web-interfaces"><a class="header" href="#web-interfaces">Web Interfaces</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Service</th><th>URL</th><th>Credentials</th></tr></thead><tbody>
<tr><td>Grafana</td><td>http://192.168.4.6:3030</td><td>admin / [.env.unraid]</td></tr>
<tr><td>Prometheus</td><td>http://192.168.4.6:9090</td><td>None</td></tr>
<tr><td>Orchestrator Docs</td><td>http://192.168.4.6:3000/docs</td><td>None</td></tr>
<tr><td>cAdvisor</td><td>http://192.168.4.6:8080</td><td>None</td></tr>
</tbody></table>
</div>
<h3 id="api-endpoints"><a class="header" href="#api-endpoints">API Endpoints</a></h3>
<p><strong>Orchestrator (Main API)</strong>:</p>
<pre><code class="language-bash"># Health check
curl http://192.168.4.6:3000/health

# API documentation
open http://192.168.4.6:3000/docs

# Submit task
curl -X POST http://192.168.4.6:3000/api/v1/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "goal": "Explain quantum computing in simple terms",
    "constraints": {"max_tokens": 500}
  }'

# Get task status
curl http://192.168.4.6:3000/api/v1/tasks/abc123
</code></pre>
<p><strong>Ollama (Local LLM)</strong>:</p>
<pre><code class="language-bash"># List models
curl http://192.168.4.6:3014/api/tags

# Generate completion
curl http://192.168.4.6:3014/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Why is the sky blue?",
  "stream": false
}'

# Chat completion
curl http://192.168.4.6:3014/api/chat -d '{
  "model": "llama3.1:8b",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ]
}'
</code></pre>
<p><strong>Prometheus (Metrics)</strong>:</p>
<pre><code class="language-bash"># Query API
curl 'http://192.168.4.6:9090/api/v1/query?query=up'

# GPU metrics
curl 'http://192.168.4.6:9090/api/v1/query?query=DCGM_FI_DEV_GPU_UTIL'
</code></pre>
<hr />
<h2 id="local-llm-usage"><a class="header" href="#local-llm-usage">Local LLM Usage</a></h2>
<h3 id="ollama-model-management"><a class="header" href="#ollama-model-management">Ollama Model Management</a></h3>
<p><strong>List installed models</strong>:</p>
<pre><code class="language-bash">docker exec octollm-ollama ollama list
</code></pre>
<p><strong>Pull new model</strong>:</p>
<pre><code class="language-bash"># Small model (&lt; 10GB)
docker exec octollm-ollama ollama pull llama3:8b

# Medium model (&lt; 30GB)
docker exec octollm-ollama ollama pull mixtral:8x7b

# Large model (requires 48GB+ VRAM or multi-GPU)
docker exec octollm-ollama ollama pull llama3:70b

# Specialized models
docker exec octollm-ollama ollama pull codellama:13b    # Code generation
docker exec octollm-ollama ollama pull nomic-embed-text # Embeddings
docker exec octollm-ollama ollama pull llama3-vision    # Image understanding
</code></pre>
<p><strong>Remove model</strong>:</p>
<pre><code class="language-bash">docker exec octollm-ollama ollama rm llama3:70b
</code></pre>
<p><strong>Model disk usage</strong>:</p>
<pre><code class="language-bash">du -sh /mnt/user/appdata/octollm/ollama/models
</code></pre>
<h3 id="recommended-models-by-use-case"><a class="header" href="#recommended-models-by-use-case">Recommended Models by Use Case</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Use Case</th><th>Model</th><th>VRAM</th><th>Speed</th><th>Quality</th></tr></thead><tbody>
<tr><td><strong>General Chat</strong></td><td>llama3.1:8b</td><td>8GB</td><td>Fast</td><td>Good</td></tr>
<tr><td><strong>Advanced Reasoning</strong></td><td>mixtral:8x7b</td><td>24GB</td><td>Medium</td><td>Excellent</td></tr>
<tr><td><strong>Code Generation</strong></td><td>codellama:13b</td><td>13GB</td><td>Medium</td><td>Excellent</td></tr>
<tr><td><strong>Code Completion</strong></td><td>codellama:7b</td><td>7GB</td><td>Fast</td><td>Good</td></tr>
<tr><td><strong>Embeddings</strong></td><td>nomic-embed-text</td><td>1GB</td><td>Very Fast</td><td>Excellent</td></tr>
<tr><td><strong>Long Context</strong></td><td>llama3-longcontext:70b</td><td>48GB</td><td>Slow</td><td>Excellent</td></tr>
</tbody></table>
</div>
<h3 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h3>
<p><strong>Concurrent requests</strong>:</p>
<pre><code class="language-bash"># .env.unraid
OLLAMA_NUM_PARALLEL=4  # Reduce if OOM errors, increase if underutilized
</code></pre>
<p><strong>Model keep-alive</strong>:</p>
<pre><code class="language-bash"># .env.unraid
OLLAMA_KEEP_ALIVE=5m   # How long to keep model in VRAM
</code></pre>
<p><strong>Max loaded models</strong>:</p>
<pre><code class="language-bash"># .env.unraid
OLLAMA_MAX_LOADED_MODELS=3  # Max models in VRAM simultaneously
</code></pre>
<h3 id="switching-between-local-and-cloud"><a class="header" href="#switching-between-local-and-cloud">Switching Between Local and Cloud</a></h3>
<p><strong>Use local LLM</strong> (default, cost-free):</p>
<pre><code class="language-bash"># .env.unraid
PREFER_LOCAL_LLM=true
</code></pre>
<p><strong>Use cloud APIs</strong> (when local unavailable):</p>
<pre><code class="language-bash"># .env.unraid
PREFER_LOCAL_LLM=false
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
</code></pre>
<p><strong>Automatic fallback</strong> (best of both worlds):</p>
<pre><code class="language-bash"># .env.unraid
PREFER_LOCAL_LLM=true
OPENAI_API_KEY=sk-proj-...  # Used only if local fails
</code></pre>
<hr />
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h3>
<h4 id="1-services-wont-start"><a class="header" href="#1-services-wont-start">1. Services Won't Start</a></h4>
<p><strong>Symptom</strong>: <code>docker-compose up -d</code> fails or services crash immediately.</p>
<p><strong>Check logs</strong>:</p>
<pre><code class="language-bash">docker-compose logs orchestrator
</code></pre>
<p><strong>Common causes</strong>:</p>
<ul>
<li>Port conflicts</li>
<li>Insufficient resources</li>
<li>Missing environment variables</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Check port availability
ss -tuln | grep -E ':(3000|3001|6001|9090)'

# Check Docker resources
docker info | grep -E "CPUs|Total Memory"

# Verify .env.unraid exists
ls -la .env.unraid

# Recreate from scratch
docker-compose down -v
bash setup-unraid.sh
</code></pre>
<h4 id="2-gpu-not-detected"><a class="header" href="#2-gpu-not-detected">2. GPU Not Detected</a></h4>
<p><strong>Symptom</strong>: <code>nvidia-smi: command not found</code> or Ollama not using GPU.</p>
<p><strong>Diagnose</strong>:</p>
<pre><code class="language-bash"># Test NVIDIA driver
nvidia-smi

# Test Docker GPU access
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi

# Check Ollama logs
docker logs octollm-ollama | grep -i gpu
</code></pre>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Reinstall NVIDIA driver plugin
# Apps → nvidia-driver → Force Update
# Reboot server

# Check Docker NVIDIA runtime
cat /etc/docker/daemon.json
# Should have "nvidia" runtime configured

# Restart Ollama with GPU
docker-compose restart ollama
</code></pre>
<h4 id="3-out-of-memory-errors"><a class="header" href="#3-out-of-memory-errors">3. Out of Memory Errors</a></h4>
<p><strong>Symptom</strong>: Containers killed with OOM, logs show memory errors.</p>
<p><strong>Check memory usage</strong>:</p>
<pre><code class="language-bash">free -h
docker stats --no-stream
</code></pre>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Reduce concurrent requests
# Edit .env.unraid:
OLLAMA_NUM_PARALLEL=2
MAX_PARALLEL_ARMS=3

# Increase container memory limits
# Edit docker-compose.unraid.yml:
services:
  ollama:
    deploy:
      resources:
        limits:
          memory: 24G  # Increase from 16G

# Use smaller models
docker exec octollm-ollama ollama pull llama3:8b
# Instead of mixtral:8x7b
</code></pre>
<h4 id="4-slow-inference"><a class="header" href="#4-slow-inference">4. Slow Inference</a></h4>
<p><strong>Symptom</strong>: LLM responses take &gt; 30 seconds.</p>
<p><strong>Check GPU usage</strong>:</p>
<pre><code class="language-bash">nvidia-smi -l 1
</code></pre>
<p><strong>If GPU usage is low</strong>:</p>
<ul>
<li>Model not loaded properly</li>
<li>CPU inference fallback</li>
<li>Queue backlog</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Force model load
docker exec octollm-ollama ollama run llama3.1:8b "Hello"

# Check Ollama logs for errors
docker logs octollm-ollama --tail=100

# Verify GPU passthrough
docker inspect octollm-ollama | grep -A5 DeviceRequests

# Restart Ollama
docker-compose restart ollama
</code></pre>
<p><strong>If GPU usage is high (100%)</strong>:</p>
<ul>
<li>Normal behavior during inference</li>
<li>Consider faster model or more GPUs</li>
<li>Reduce parallel requests</li>
</ul>
<h4 id="5-database-connection-errors"><a class="header" href="#5-database-connection-errors">5. Database Connection Errors</a></h4>
<p><strong>Symptom</strong>: Services can't connect to PostgreSQL/Redis.</p>
<p><strong>Check database health</strong>:</p>
<pre><code class="language-bash">docker-compose ps postgres redis
docker logs octollm-postgres --tail=50
docker logs octollm-redis --tail=50
</code></pre>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Wait for health checks
docker-compose ps  # Check health status

# Manual health check
docker exec octollm-postgres pg_isready -U octollm
docker exec octollm-redis redis-cli ping

# Restart databases
docker-compose restart postgres redis

# Check network connectivity
docker exec octollm-orchestrator ping postgres
docker exec octollm-orchestrator ping redis
</code></pre>
<h4 id="6-port-conflicts"><a class="header" href="#6-port-conflicts">6. Port Conflicts</a></h4>
<p><strong>Symptom</strong>: "bind: address already in use"</p>
<p><strong>Find conflicting process</strong>:</p>
<pre><code class="language-bash">ss -tuln | grep :3000
lsof -i :3000
</code></pre>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Stop conflicting service
docker stop conflicting-container
# Or change OctoLLM ports in docker-compose.unraid.yml

# Use alternative ports
# Edit docker-compose.unraid.yml:
services:
  orchestrator:
    ports:
      - "8000:8000"  # Changed from 3000
</code></pre>
<h3 id="logging-and-debugging"><a class="header" href="#logging-and-debugging">Logging and Debugging</a></h3>
<p><strong>Enable debug logging</strong>:</p>
<pre><code class="language-bash"># Edit .env.unraid
LOG_LEVEL=DEBUG
RUST_LOG=debug
RUST_BACKTRACE=1

# Restart services
docker-compose restart
</code></pre>
<p><strong>View aggregated logs</strong>:</p>
<pre><code class="language-bash"># All services, follow mode
docker-compose logs -f

# Specific time range
docker-compose logs --since="2024-01-15T10:00:00"

# Filter by keyword
docker-compose logs | grep ERROR
</code></pre>
<p><strong>Access container shell</strong>:</p>
<pre><code class="language-bash"># Orchestrator (Python)
docker exec -it octollm-orchestrator bash

# Ollama (check models)
docker exec -it octollm-ollama bash
ls -lh /root/.ollama/models
</code></pre>
<p><strong>Check resource usage</strong>:</p>
<pre><code class="language-bash"># Real-time stats
docker stats

# Per-container stats
docker stats octollm-ollama

# Custom monitoring script
bash scripts/monitor-resources.sh
</code></pre>
<h3 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h3>
<ol>
<li><strong>Check logs first</strong>: <code>docker-compose logs [service]</code></li>
<li><strong>Search GitHub issues</strong>: https://github.com/your-org/octollm/issues</li>
<li><strong>Ask in discussions</strong>: https://github.com/your-org/octollm/discussions</li>
<li><strong>Unraid forum</strong>: https://forums.unraid.net</li>
</ol>
<p><strong>When reporting issues, include</strong>:</p>
<ul>
<li>Unraid version: <code>cat /etc/unraid-version</code></li>
<li>Hardware specs: CPU, RAM, GPU</li>
<li>Docker version: <code>docker --version</code></li>
<li>Logs: <code>docker-compose logs [service] --tail=100</code></li>
<li>Config: <code>.env.unraid</code> (redact passwords!)</li>
</ul>
<hr />
<h2 id="backup--restore"><a class="header" href="#backup--restore">Backup &amp; Restore</a></h2>
<h3 id="automated-backup"><a class="header" href="#automated-backup">Automated Backup</a></h3>
<p><strong>Run backup script</strong>:</p>
<pre><code class="language-bash">cd /mnt/user/appdata/octollm/infrastructure/unraid
bash scripts/backup-data.sh
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Starting OctoLLM backup...
Timestamp: 20250112_143022
Stopping services...
Backing up PostgreSQL...
Backing up data directories...
Backup complete!
  PostgreSQL: 150M
  Data files: 2.5G
  Location: /mnt/user/backups/octollm
Restarting services...
Done!
</code></pre>
<p><strong>Backup location</strong>:</p>
<pre><code>/mnt/user/backups/octollm/
├── octollm_backup_20250112_143022_postgres.sql
└── octollm_backup_20250112_143022_data.tar.gz
</code></pre>
<h3 id="manual-backup"><a class="header" href="#manual-backup">Manual Backup</a></h3>
<p><strong>PostgreSQL only</strong>:</p>
<pre><code class="language-bash">docker exec octollm-postgres pg_dumpall -U octollm &gt; backup_$(date +%Y%m%d).sql
</code></pre>
<p><strong>Data directories</strong>:</p>
<pre><code class="language-bash">tar -czf octollm_data_$(date +%Y%m%d).tar.gz \
  -C /mnt/user/appdata \
  --exclude='octollm/ollama/models' \
  octollm/
</code></pre>
<p><strong>Ollama models</strong> (optional, large):</p>
<pre><code class="language-bash">tar -czf octollm_models_$(date +%Y%m%d).tar.gz \
  -C /mnt/user/appdata/octollm/ollama \
  models/
</code></pre>
<h3 id="restore-from-backup"><a class="header" href="#restore-from-backup">Restore from Backup</a></h3>
<p><strong>Step 1: Stop services</strong>:</p>
<pre><code class="language-bash">cd /mnt/user/appdata/octollm/infrastructure/unraid
docker-compose down
</code></pre>
<p><strong>Step 2: Restore data directories</strong>:</p>
<pre><code class="language-bash">cd /mnt/user/appdata
tar -xzf /mnt/user/backups/octollm/octollm_backup_20250112_143022_data.tar.gz
</code></pre>
<p><strong>Step 3: Restore PostgreSQL</strong>:</p>
<pre><code class="language-bash">docker-compose up -d postgres
sleep 10
docker exec -i octollm-postgres psql -U octollm &lt; /mnt/user/backups/octollm/octollm_backup_20250112_143022_postgres.sql
</code></pre>
<p><strong>Step 4: Restart all services</strong>:</p>
<pre><code class="language-bash">docker-compose up -d
</code></pre>
<h3 id="backup-schedule"><a class="header" href="#backup-schedule">Backup Schedule</a></h3>
<p><strong>Unraid User Scripts plugin</strong> (recommended):</p>
<ol>
<li>Install "User Scripts" plugin from Community Applications</li>
<li>Add new script:</li>
</ol>
<pre><code class="language-bash">#!/bin/bash
cd /mnt/user/appdata/octollm/infrastructure/unraid
bash scripts/backup-data.sh

# Optional: Keep only last 7 backups
find /mnt/user/backups/octollm -type f -mtime +7 -delete
</code></pre>
<ol start="3">
<li>Schedule: Daily at 2:00 AM</li>
</ol>
<h3 id="cloud-backup"><a class="header" href="#cloud-backup">Cloud Backup</a></h3>
<p><strong>Sync to cloud storage</strong>:</p>
<pre><code class="language-bash"># AWS S3
aws s3 sync /mnt/user/backups/octollm s3://my-bucket/octollm-backups/

# Google Cloud Storage
gsutil -m rsync -r /mnt/user/backups/octollm gs://my-bucket/octollm-backups/

# Rclone (any provider)
rclone sync /mnt/user/backups/octollm remote:octollm-backups/
</code></pre>
<hr />
<h2 id="performance-tuning-1"><a class="header" href="#performance-tuning-1">Performance Tuning</a></h2>
<h3 id="cpu-pinning-numa-optimization"><a class="header" href="#cpu-pinning-numa-optimization">CPU Pinning (NUMA Optimization)</a></h3>
<p>Dell PowerEdge R730xd has 2 NUMA nodes. Pin containers to specific nodes for better performance.</p>
<p><strong>Check NUMA topology</strong>:</p>
<pre><code class="language-bash">lscpu | grep NUMA
numactl --hardware
</code></pre>
<p><strong>Edit docker-compose.unraid.yml</strong>:</p>
<pre><code class="language-yaml">services:
  ollama:
    cpuset: "0-15,32-47"  # NUMA node 0
    mem: "0"              # NUMA node 0 memory

  orchestrator:
    cpuset: "16-31,48-63" # NUMA node 1
    mem: "1"              # NUMA node 1 memory
</code></pre>
<h3 id="postgresql-tuning"><a class="header" href="#postgresql-tuning">PostgreSQL Tuning</a></h3>
<p><strong>Create custom config</strong>:</p>
<pre><code class="language-bash">cat &gt; /mnt/user/appdata/octollm/postgres/postgresql.conf &lt;&lt; EOF
# OctoLLM PostgreSQL Performance Tuning

# Memory
shared_buffers = 2GB                  # 25% of dedicated RAM
effective_cache_size = 8GB            # 50% of system RAM
work_mem = 64MB                       # Per query operation
maintenance_work_mem = 512MB          # VACUUM, CREATE INDEX

# Connections
max_connections = 200

# Query Planner
random_page_cost = 1.1               # SSD optimization
effective_io_concurrency = 200       # SSD parallel I/O

# WAL
wal_buffers = 16MB
checkpoint_completion_target = 0.9
max_wal_size = 4GB
min_wal_size = 1GB

# Logging
log_destination = 'stderr'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y%m%d.log'
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_statement = 'none'               # 'all' for debugging
log_duration = off
log_min_duration_statement = 1000    # Log slow queries (&gt; 1s)
EOF
</code></pre>
<p><strong>Mount in docker-compose.unraid.yml</strong>:</p>
<pre><code class="language-yaml">services:
  postgres:
    volumes:
      - /mnt/user/appdata/octollm/postgres/postgresql.conf:/var/lib/postgresql/data/postgresql.conf:ro
    command: postgres -c config_file=/var/lib/postgresql/data/postgresql.conf
</code></pre>
<h3 id="redis-tuning"><a class="header" href="#redis-tuning">Redis Tuning</a></h3>
<p><strong>Edit .env.unraid</strong>:</p>
<pre><code class="language-bash"># Redis Configuration
REDIS_MAXMEMORY=4gb
REDIS_MAXMEMORY_POLICY=allkeys-lru

# Persistence (reduce writes for performance)
REDIS_SAVE_SECONDS=900 1            # Save after 15 min if 1+ key changed
REDIS_SAVE_SECONDS_2=300 10         # Save after 5 min if 10+ keys changed
</code></pre>
<h3 id="ollama-gpu-performance"><a class="header" href="#ollama-gpu-performance">Ollama GPU Performance</a></h3>
<p><strong>Maximize throughput</strong>:</p>
<pre><code class="language-bash"># .env.unraid
OLLAMA_NUM_PARALLEL=4              # Max concurrent requests (GPU memory limited)
OLLAMA_KEEP_ALIVE=10m              # Keep models loaded longer
OLLAMA_MAX_LOADED_MODELS=2         # Reduce model swapping
</code></pre>
<p><strong>Power limit</strong> (Tesla P40 defaults to 250W):</p>
<pre><code class="language-bash"># Increase to maximum (if cooling allows)
nvidia-smi -pl 250

# Monitor temperature
nvidia-smi -l 1
# Should stay below 85°C
</code></pre>
<h3 id="network-optimization"><a class="header" href="#network-optimization">Network Optimization</a></h3>
<p><strong>MTU tuning</strong> (for 4Gbps bond):</p>
<pre><code class="language-bash"># Check current MTU
ip link show bond0

# Increase MTU (if switch supports)
ifconfig bond0 mtu 9000

# Test with jumbo frames
ping -M do -s 8972 192.168.4.6
</code></pre>
<p><strong>Docker network tuning</strong>:</p>
<pre><code class="language-bash"># Edit docker-compose.unraid.yml
networks:
  octollm-net:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 9000  # Jumbo frames
</code></pre>
<hr />
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<h3 id="grafana-dashboards"><a class="header" href="#grafana-dashboards">Grafana Dashboards</a></h3>
<p><strong>Access Grafana</strong>:</p>
<ul>
<li>URL: http://192.168.4.6:3030</li>
<li>Username: admin</li>
<li>Password: [from .env.unraid]</li>
</ul>
<p><strong>Pre-configured dashboards</strong>:</p>
<ol>
<li>
<p><strong>OctoLLM Unraid Dashboard</strong> (default)</p>
<ul>
<li>System overview (CPU, RAM, disk, network)</li>
<li>GPU metrics (utilization, temperature, memory, power)</li>
<li>Service health status</li>
<li>Database performance</li>
<li>Ollama LLM metrics</li>
<li>Container resources</li>
</ul>
</li>
<li>
<p><strong>Import additional dashboards</strong>:</p>
<ul>
<li>Click "+ → Import"</li>
<li>Enter dashboard ID or upload JSON</li>
<li>Recommended IDs:
<ul>
<li>1860: Node Exporter Full</li>
<li>179: Docker Host &amp; Container Overview</li>
<li>12321: NVIDIA DCGM Exporter</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="prometheus-alerts"><a class="header" href="#prometheus-alerts">Prometheus Alerts</a></h3>
<p><strong>View alerts</strong>:</p>
<ul>
<li>URL: http://192.168.4.6:9090/alerts</li>
</ul>
<p><strong>Alert rules</strong> (from <code>prometheus/alerts.unraid.yml</code>):</p>
<ul>
<li>High CPU usage (&gt; 80%)</li>
<li>High memory usage (&gt; 85%)</li>
<li>Low disk space (&lt; 10%)</li>
<li>High GPU temperature (&gt; 80°C)</li>
<li>Service down</li>
<li>Database connection exhaustion</li>
<li>High error rate</li>
</ul>
<p><strong>Configure alerting</strong> (Slack, email, PagerDuty):</p>
<p>Edit <code>/mnt/user/appdata/octollm/prometheus/config/prometheus.yml</code>:</p>
<pre><code class="language-yaml">alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - 'alertmanager:9093'
</code></pre>
<p>Deploy Alertmanager:</p>
<pre><code class="language-yaml"># Add to docker-compose.unraid.yml
services:
  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
</code></pre>
<h3 id="real-time-monitoring"><a class="header" href="#real-time-monitoring">Real-Time Monitoring</a></h3>
<p><strong>Custom monitoring script</strong>:</p>
<pre><code class="language-bash">bash scripts/monitor-resources.sh
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>╔════════════════════════════════════════════════════════════════════════════╗
║  OctoLLM Resource Monitor - tower
║  Uptime: up 5 days, 12 hours
╚════════════════════════════════════════════════════════════════════════════╝

CPU (64 cores): 45.2%
[██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░]

RAM (504GB): 125GB / 504GB (24.8%)
[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
NVIDIA Tesla P40 GPU
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Utilization:  87%
VRAM:         18432MB / 24576MB (75.0%)
Temperature:  72°C
Power:        187W / 250W

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Storage (/mnt/user)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Usage: 93TB / 144TB (64%)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Network (bond0 - 4Gbps)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Download: 42 MB/s  |  Upload: 18 MB/s
</code></pre>
<h3 id="logging"><a class="header" href="#logging">Logging</a></h3>
<p><strong>View logs in Grafana</strong> (Loki integration):</p>
<ul>
<li>Navigate to Explore</li>
<li>Select "Loki" datasource</li>
<li>Query: <code>{container_name=~"octollm-.*"}</code></li>
</ul>
<p><strong>Command-line log access</strong>:</p>
<pre><code class="language-bash"># Real-time logs
docker-compose logs -f orchestrator

# Search logs
docker-compose logs orchestrator | grep ERROR

# Export logs
docker-compose logs --no-color &gt; octollm-logs-$(date +%Y%m%d).txt
</code></pre>
<hr />
<h2 id="security"><a class="header" href="#security">Security</a></h2>
<h3 id="network-isolation"><a class="header" href="#network-isolation">Network Isolation</a></h3>
<p><strong>Firewall rules</strong> (iptables):</p>
<pre><code class="language-bash"># Allow from local network only
iptables -A INPUT -p tcp -s 192.168.0.0/16 --dport 3000:9999 -j ACCEPT

# Block from internet
iptables -A INPUT -p tcp --dport 3000:9999 -j DROP

# Save rules (Unraid persists in /boot/config/network.cfg)
iptables-save &gt; /boot/config/firewall-rules
</code></pre>
<p><strong>Docker network isolation</strong>:</p>
<pre><code class="language-yaml"># docker-compose.unraid.yml
networks:
  octollm-net:
    driver: bridge
    internal: false  # Set to true to disable internet access
    ipam:
      config:
        - subnet: 172.20.0.0/16
</code></pre>
<h3 id="vpn-access-recommended"><a class="header" href="#vpn-access-recommended">VPN Access (Recommended)</a></h3>
<p><strong>Option 1: Tailscale</strong> (easiest):</p>
<pre><code class="language-bash"># Install Tailscale on Unraid
curl -fsSL https://tailscale.com/install.sh | sh

# Authenticate
tailscale up

# Access from anywhere
# http://tower.tail-scale.ts.net:3000
</code></pre>
<p><strong>Option 2: WireGuard</strong> (manual):</p>
<ul>
<li>Install WireGuard plugin from Community Applications</li>
<li>Configure peer</li>
<li>Access via VPN tunnel</li>
</ul>
<h3 id="secrets-management"><a class="header" href="#secrets-management">Secrets Management</a></h3>
<p><strong>Never commit these files</strong>:</p>
<ul>
<li><code>.env.unraid</code></li>
<li><code>.env.unraid.backup</code></li>
<li><code>backups/*.sql</code></li>
</ul>
<p><strong>Verify gitignore</strong>:</p>
<pre><code class="language-bash">cd /mnt/user/appdata/octollm
git status --ignored
# Should NOT list .env.unraid
</code></pre>
<p><strong>Rotate passwords regularly</strong>:</p>
<pre><code class="language-bash"># Regenerate all passwords
cd infrastructure/unraid
bash setup-unraid.sh
# Answer "y" when prompted to overwrite .env.unraid
</code></pre>
<h3 id="tlsssl-production"><a class="header" href="#tlsssl-production">TLS/SSL (Production)</a></h3>
<p><strong>Behind reverse proxy</strong> (NGINX Proxy Manager):</p>
<ol>
<li>Install NGINX Proxy Manager from Community Applications</li>
<li>Create proxy host:
<ul>
<li>Domain: octollm.yourdomain.com</li>
<li>Forward to: 192.168.4.6:3000</li>
<li>Enable SSL (Let's Encrypt)</li>
</ul>
</li>
<li>Access via: https://octollm.yourdomain.com</li>
</ol>
<p><strong>Direct TLS</strong> (advanced):</p>
<pre><code class="language-bash"># Generate self-signed cert
openssl req -x509 -newkey rsa:4096 -nodes \
  -keyout /mnt/user/appdata/octollm/certs/key.pem \
  -out /mnt/user/appdata/octollm/certs/cert.pem \
  -days 365

# Edit .env.unraid
ENABLE_TLS=true
TLS_CERT_PATH=/mnt/user/appdata/octollm/certs/cert.pem
TLS_KEY_PATH=/mnt/user/appdata/octollm/certs/key.pem
</code></pre>
<h3 id="audit-logging"><a class="header" href="#audit-logging">Audit Logging</a></h3>
<p><strong>PostgreSQL audit table</strong> (already created by setup):</p>
<pre><code class="language-sql">SELECT * FROM audit.api_logs
ORDER BY timestamp DESC
LIMIT 100;
</code></pre>
<p><strong>Query audit logs</strong>:</p>
<pre><code class="language-bash">docker exec -it octollm-postgres psql -U octollm -c "
SELECT
  timestamp,
  endpoint,
  method,
  status_code,
  user_id,
  ip_address
FROM audit.api_logs
WHERE timestamp &gt; NOW() - INTERVAL '1 hour'
ORDER BY timestamp DESC;
"
</code></pre>
<hr />
<h2 id="migration-to-cloud"><a class="header" href="#migration-to-cloud">Migration to Cloud</a></h2>
<p>When ready to deploy to production (GKE/EKS):</p>
<h3 id="step-1-export-data"><a class="header" href="#step-1-export-data">Step 1: Export Data</a></h3>
<pre><code class="language-bash"># Backup all data
cd /mnt/user/appdata/octollm/infrastructure/unraid
bash scripts/backup-data.sh

# Upload to cloud storage
aws s3 cp /mnt/user/backups/octollm/ s3://my-bucket/octollm-migration/ --recursive
</code></pre>
<h3 id="step-2-update-configuration"><a class="header" href="#step-2-update-configuration">Step 2: Update Configuration</a></h3>
<p><strong>Switch to cloud LLMs</strong>:</p>
<pre><code class="language-bash"># .env.cloud
PREFER_LOCAL_LLM=false
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
</code></pre>
<p><strong>Use managed databases</strong>:</p>
<pre><code class="language-bash"># .env.cloud
DATABASE_URL=postgresql://user:pass@cloud-sql-instance:5432/octollm
REDIS_URL=redis://redis-memorystore:6379
QDRANT_URL=https://my-cluster.qdrant.io
</code></pre>
<h3 id="step-3-deploy-to-kubernetes"><a class="header" href="#step-3-deploy-to-kubernetes">Step 3: Deploy to Kubernetes</a></h3>
<pre><code class="language-bash">cd /mnt/user/appdata/octollm/infrastructure/kubernetes

# Apply namespace
kubectl apply -f namespaces/octollm-prod-namespace.yaml

# Deploy with Helm (recommended)
helm install octollm ./charts/octollm \
  --namespace octollm-prod \
  --values ./charts/octollm/values-prod.yaml

# Or apply manifests directly
kubectl apply -k overlays/prod
</code></pre>
<h3 id="step-4-data-migration"><a class="header" href="#step-4-data-migration">Step 4: Data Migration</a></h3>
<p><strong>PostgreSQL</strong>:</p>
<pre><code class="language-bash"># Restore to Cloud SQL
cat backup_postgres.sql | psql "$DATABASE_URL"
</code></pre>
<p><strong>Qdrant vectors</strong>:</p>
<pre><code class="language-bash"># Use Qdrant snapshot API
curl -X POST http://192.168.4.6:3012/collections/octollm/snapshots
curl -X GET http://192.168.4.6:3012/collections/octollm/snapshots/snapshot_name/download &gt; snapshot.tar

# Upload to Qdrant Cloud
curl -X POST https://my-cluster.qdrant.io/collections/octollm/snapshots/upload \
  -F "snapshot=@snapshot.tar"
</code></pre>
<h3 id="cost-comparison"><a class="header" href="#cost-comparison">Cost Comparison</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Component</th><th>Unraid (Monthly)</th><th>GKE (Monthly)</th><th>Difference</th></tr></thead><tbody>
<tr><td>Compute</td><td>$0 (owned)</td><td>$200-500</td><td>+$200-500</td></tr>
<tr><td>LLM APIs</td><td>$0 (local)</td><td>$150-700</td><td>+$150-700</td></tr>
<tr><td>Databases</td><td>$0</td><td>$100-300</td><td>+$100-300</td></tr>
<tr><td>Storage</td><td>$0</td><td>$20-50</td><td>+$20-50</td></tr>
<tr><td>Networking</td><td>$0</td><td>$50-100</td><td>+$50-100</td></tr>
<tr><td><strong>Total</strong></td><td><strong>~$50 electricity</strong></td><td><strong>$520-1,650</strong></td><td><strong>+$470-1,600/mo</strong></td></tr>
</tbody></table>
</div>
<p><strong>Break-even analysis</strong>:</p>
<ul>
<li>Development on Unraid: ~$50/month</li>
<li>Production on GKE: ~$1,000/month</li>
<li><strong>Savings during development</strong>: $950/month × 6 months = $5,700</li>
</ul>
<p>See full <a href="./cloud-migration-from-unraid.html">Cloud Migration Guide</a> for detailed steps.</p>
<hr />
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>You now have a fully functional OctoLLM deployment on Unraid with:</p>
<p>✅ GPU-accelerated local LLM inference (Tesla P40)
✅ Complete monitoring stack (Prometheus, Grafana, Loki)
✅ Automated backups and health checks
✅ Production-ready architecture
✅ Cost savings: $150-700/month in LLM API fees</p>
<h3 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h3>
<ol>
<li><strong>Explore API</strong>: http://192.168.4.6:3000/docs</li>
<li><strong>Monitor with Grafana</strong>: http://192.168.4.6:3030</li>
<li><strong>Submit test tasks</strong>: See API examples above</li>
<li><strong>Optimize performance</strong>: Tune based on your workload</li>
<li><strong>Join community</strong>: https://github.com/your-org/octollm/discussions</li>
</ol>
<h3 id="support"><a class="header" href="#support">Support</a></h3>
<ul>
<li><strong>Documentation</strong>: https://github.com/your-org/octollm/docs</li>
<li><strong>Issues</strong>: https://github.com/your-org/octollm/issues</li>
<li><strong>Discord</strong>: https://discord.gg/octollm</li>
<li><strong>Email</strong>: support@octollm.io</li>
</ul>
<hr />
<p><strong>Last Updated</strong>: 2025-11-12
<strong>Version</strong>: 1.0.0
<strong>Tested On</strong>: Unraid 7.2.0, Dell PowerEdge R730xd, Tesla P40</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../operations/docker-compose-setup.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../operations/monitoring-alerting.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../operations/docker-compose-setup.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../operations/monitoring-alerting.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
