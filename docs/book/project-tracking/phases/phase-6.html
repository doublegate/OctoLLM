<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Phase 6: Production - OctoLLM Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Distributed AI Architecture for Offensive Security and Developer Tooling - Comprehensive technical documentation covering architecture, API, development, operations, and security.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">OctoLLM Documentation</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/doublegate/OctoLLM" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/doublegate/OctoLLM/edit/main/docs/src/project-tracking/phases/phase-6.md" title="Suggest an edit" aria-label="Suggest an edit" rel="edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="phase-6-production-readiness"><a class="header" href="#phase-6-production-readiness">Phase 6: Production Readiness</a></h1>
<p><strong>Status</strong>: Not Started
<strong>Duration</strong>: 8-10 weeks
<strong>Team Size</strong>: 4-5 engineers (1 SRE, 1 ML engineer, 1 Python, 1 Rust, 1 DevOps)
<strong>Prerequisites</strong>: Phase 5 complete (security hardening)
<strong>Start Date</strong>: TBD
<strong>Target Completion</strong>: TBD</p>
<hr />
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Phase 6 prepares OctoLLM for production deployment at scale with autoscaling, cost optimization, compliance implementation, advanced performance tuning, and multi-tenancy support.</p>
<p><strong>Key Deliverables</strong>:</p>
<ol>
<li>Autoscaling - HorizontalPodAutoscaler with custom metrics, VPA, cluster autoscaling</li>
<li>Cost Optimization - Right-sizing, spot instances, reserved capacity, LLM cost reduction</li>
<li>Compliance - SOC 2 Type II, ISO 27001, GDPR, CCPA, HIPAA readiness</li>
<li>Advanced Performance - Rust rewrites, model fine-tuning, advanced caching, speculative execution</li>
<li>Multi-Tenancy - Tenant isolation, authentication, data isolation, usage-based billing</li>
</ol>
<p><strong>Success Criteria</strong>:</p>
<ul>
<li>✅ Autoscaling handles 10x traffic spikes without degradation</li>
<li>✅ Cost per task reduced by 50% vs Phase 5</li>
<li>✅ SOC 2 Type II audit passed</li>
<li>✅ P99 latency &lt;10s for critical tasks (vs &lt;30s in Phase 1)</li>
<li>✅ Multi-tenant isolation tested and verified</li>
<li>✅ Production SLA: 99.9% uptime, &lt;15s P95 latency</li>
<li>✅ Zero customer-impacting security incidents in first 90 days</li>
</ul>
<p><strong>Reference</strong>: <code>docs/doc_phases/PHASE-6-COMPLETE-SPECIFICATIONS.md</code> (14,000+ lines)</p>
<hr />
<h2 id="sprint-61-autoscaling-week-33-34"><a class="header" href="#sprint-61-autoscaling-week-33-34">Sprint 6.1: Autoscaling [Week 33-34]</a></h2>
<p><strong>Duration</strong>: 2 weeks
<strong>Team</strong>: 2 engineers (1 SRE, 1 DevOps)
<strong>Prerequisites</strong>: Phase 3 complete (Kubernetes deployment)
<strong>Priority</strong>: HIGH</p>
<h3 id="sprint-goals"><a class="header" href="#sprint-goals">Sprint Goals</a></h3>
<ul>
<li>Implement HorizontalPodAutoscaler (HPA) for all services</li>
<li>Configure VerticalPodAutoscaler (VPA) for right-sizing</li>
<li>Set up cluster autoscaling for node pools</li>
<li>Create custom metrics for LLM workload scaling</li>
<li>Test autoscaling under load</li>
<li>Document scaling policies and runbooks</li>
</ul>
<h3 id="architecture-decisions"><a class="header" href="#architecture-decisions">Architecture Decisions</a></h3>
<p><strong>Scaling Strategy</strong>: Hybrid approach (HPA for replicas, VPA for resource requests, cluster autoscaler for nodes)
<strong>Metrics</strong>: CPU, memory, custom (queue depth, task latency, LLM token rate)
<strong>Target Utilization</strong>: 70% CPU/memory (allows headroom for spikes)
<strong>Scale-Up Policy</strong>: Aggressive (30s stabilization)
<strong>Scale-Down Policy</strong>: Conservative (5 minutes stabilization to prevent flapping)
<strong>Min/Max Replicas</strong>: Service-dependent (orchestrator: 3-20, arms: 2-10)</p>
<h3 id="tasks"><a class="header" href="#tasks">Tasks</a></h3>
<h4 id="horizontalpodautoscaler-setup-10-hours"><a class="header" href="#horizontalpodautoscaler-setup-10-hours">HorizontalPodAutoscaler Setup (10 hours)</a></h4>
<ul>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Install Metrics Server</strong> (1 hour)</p>
<ul>
<li>Deploy metrics-server in kube-system namespace</li>
<li>Verify metric collection</li>
<li>Code example:
<pre><code class="language-bash"># Install metrics-server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Verify metrics available
kubectl top nodes
kubectl top pods -n octollm
</code></pre>
</li>
<li>Files to create: <code>k8s/monitoring/metrics-server.yaml</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Create HPA for Orchestrator</strong> (2 hours)</p>
<ul>
<li>Scale based on CPU and custom metrics (task queue depth)</li>
<li>Aggressive scale-up, conservative scale-down</li>
<li>Code example:
<pre><code class="language-yaml"># k8s/autoscaling/orchestrator-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: orchestrator-hpa
  namespace: octollm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: orchestrator
  minReplicas: 3
  maxReplicas: 20
  metrics:
  # CPU-based scaling
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # Memory-based scaling
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 75

  # Custom metric: task queue depth
  - type: Pods
    pods:
      metric:
        name: task_queue_depth
      target:
        type: AverageValue
        averageValue: "10"  # Scale up if &gt;10 tasks per pod

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100  # Double replicas
        periodSeconds: 30
      - type: Pods
        value: 4  # Or add 4 pods
        periodSeconds: 30
      selectPolicy: Max  # Choose most aggressive

    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
      - type: Percent
        value: 50  # Remove 50% of pods
        periodSeconds: 60
      - type: Pods
        value: 2  # Or remove 2 pods
        periodSeconds: 60
      selectPolicy: Min  # Choose most conservative
</code></pre>
</li>
<li>Files to create: <code>k8s/autoscaling/orchestrator-hpa.yaml</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Create HPAs for All Arms</strong> (4 hours)</p>
<ul>
<li>Planner Arm: Scale on CPU + task decomposition requests</li>
<li>Executor Arm: Scale on CPU + active executions</li>
<li>Coder Arm: Scale on CPU + code generation requests</li>
<li>Judge Arm: Scale on CPU + validation requests</li>
<li>Safety Guardian Arm: Scale on CPU + PII detection requests</li>
<li>Retriever Arm: Scale on CPU + search requests</li>
<li>Code example (Executor Arm):
<pre><code class="language-yaml"># k8s/autoscaling/executor-arm-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: executor-arm-hpa
  namespace: octollm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: executor-arm
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  - type: Pods
    pods:
      metric:
        name: active_executions
      target:
        type: AverageValue
        averageValue: "3"  # Max 3 concurrent executions per pod

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
</code></pre>
</li>
<li>Files to create: <code>k8s/autoscaling/executor-arm-hpa.yaml</code>, similar for other arms</li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Implement Custom Metrics Exporter</strong> (3 hours)</p>
<ul>
<li>
<p>Expose application metrics for HPA (task queue depth, active executions)</p>
</li>
<li>
<p>Use Prometheus adapter</p>
</li>
<li>
<p>Code example:</p>
<pre><code class="language-python"># orchestrator/metrics/custom_metrics.py
from prometheus_client import Gauge
from typing import Dict, Any

# Define custom metrics for autoscaling
task_queue_depth_gauge = Gauge(
    'task_queue_depth',
    'Number of tasks waiting in queue per pod',
    ['pod_name']
)

active_tasks_gauge = Gauge(
    'active_tasks',
    'Number of tasks currently being processed',
    ['pod_name']
)

class CustomMetricsExporter:
    """Export custom metrics for HPA."""

    def __init__(self, pod_name: str):
        self.pod_name = pod_name

    def update_queue_depth(self, depth: int):
        """Update task queue depth metric."""
        task_queue_depth_gauge.labels(pod_name=self.pod_name).set(depth)

    def update_active_tasks(self, count: int):
        """Update active task count metric."""
        active_tasks_gauge.labels(pod_name=self.pod_name).set(count)
</code></pre>
<pre><code class="language-yaml"># k8s/monitoring/prometheus-adapter-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-adapter-config
  namespace: monitoring
data:
  config.yaml: |
    rules:
    - seriesQuery: 'task_queue_depth{namespace="octollm"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod_name: {resource: "pod"}
      name:
        matches: "^(.*)$"
        as: "task_queue_depth"
      metricsQuery: 'avg_over_time(task_queue_depth{&lt;&lt;.LabelMatchers&gt;&gt;}[1m])'

    - seriesQuery: 'active_executions{namespace="octollm"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod_name: {resource: "pod"}
      name:
        matches: "^(.*)$"
        as: "active_executions"
      metricsQuery: 'avg_over_time(active_executions{&lt;&lt;.LabelMatchers&gt;&gt;}[1m])'
</code></pre>
</li>
<li>
<p>Files to create: <code>orchestrator/metrics/custom_metrics.py</code>, <code>k8s/monitoring/prometheus-adapter-config.yaml</code></p>
</li>
</ul>
</li>
</ul>
<h4 id="verticalpodautoscaler-setup-4-hours"><a class="header" href="#verticalpodautoscaler-setup-4-hours">VerticalPodAutoscaler Setup (4 hours)</a></h4>
<ul>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Install VPA</strong> (1 hour)</p>
<ul>
<li>Deploy VPA components (recommender, updater, admission controller)</li>
<li>Code example:
<pre><code class="language-bash"># Install VPA
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
</code></pre>
</li>
<li>Files to create: <code>k8s/autoscaling/vpa-install.sh</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Create VPA Policies</strong> (2 hours)</p>
<ul>
<li>Recommendation-only mode for initial analysis</li>
<li>Auto mode for non-critical services</li>
<li>Code example:
<pre><code class="language-yaml"># k8s/autoscaling/orchestrator-vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: orchestrator-vpa
  namespace: octollm
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: orchestrator
  updatePolicy:
    updateMode: "Auto"  # Auto, Recreate, Initial, or Off
  resourcePolicy:
    containerPolicies:
    - containerName: orchestrator
      minAllowed:
        cpu: 500m
        memory: 1Gi
      maxAllowed:
        cpu: 8000m
        memory: 16Gi
      controlledResources:
      - cpu
      - memory
</code></pre>
</li>
<li>Files to create: <code>k8s/autoscaling/orchestrator-vpa.yaml</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Monitor VPA Recommendations</strong> (1 hour)</p>
<ul>
<li>Analyze recommendations for all services</li>
<li>Adjust resource requests based on data</li>
<li>Code example:
<pre><code class="language-bash"># scripts/analyze_vpa_recommendations.sh
#!/bin/bash
set -e

echo "=== VPA Recommendations Analysis ==="

for deployment in orchestrator planner-arm executor-arm coder-arm judge-arm safety-guardian-arm retriever-arm; do
    echo "\n--- $deployment ---"

    # Get VPA recommendations
    kubectl get vpa ${deployment}-vpa -n octollm -o json | \
        jq -r '.status.recommendation.containerRecommendations[] |
               "Container: \(.containerName)\n  Current CPU: \(.target.cpu)\n  Recommended CPU: \(.upperBound.cpu)\n  Current Memory: \(.target.memory)\n  Recommended Memory: \(.upperBound.memory)"'
done
</code></pre>
</li>
<li>Files to create: <code>scripts/analyze_vpa_recommendations.sh</code></li>
</ul>
</li>
</ul>
<h4 id="cluster-autoscaler-setup-4-hours"><a class="header" href="#cluster-autoscaler-setup-4-hours">Cluster Autoscaler Setup (4 hours)</a></h4>
<ul>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Configure Cluster Autoscaler</strong> (2 hours)</p>
<ul>
<li>Set up node pools with min/max sizes</li>
<li>Configure autoscaler for each cloud provider</li>
<li>Code example (GKE):
<pre><code class="language-yaml"># k8s/autoscaling/cluster-autoscaler-gke.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  template:
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.28.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=gce
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=mig:namePrefix=octollm-node-pool
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
        - --scale-down-delay-after-add=5m
        - --scale-down-unneeded-time=5m
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
rules:
- apiGroups: [""]
  resources: ["events", "endpoints"]
  verbs: ["create", "patch"]
- apiGroups: [""]
  resources: ["pods/eviction"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["pods/status"]
  verbs: ["update"]
- apiGroups: [""]
  resources: ["endpoints"]
  resourceNames: ["cluster-autoscaler"]
  verbs: ["get", "update"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["watch", "list", "get", "update"]
- apiGroups: [""]
  resources: ["pods", "services", "replicationcontrollers", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["extensions"]
  resources: ["replicasets", "daemonsets"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["watch", "list"]
- apiGroups: ["apps"]
  resources: ["statefulsets", "replicasets", "daemonsets"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "patch"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["create"]
- apiGroups: ["coordination.k8s.io"]
  resourceNames: ["cluster-autoscaler"]
  resources: ["leases"]
  verbs: ["get", "update"]
</code></pre>
</li>
<li>Files to create: <code>k8s/autoscaling/cluster-autoscaler-gke.yaml</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Create Node Pools with Labels</strong> (1 hour)</p>
<ul>
<li>Separate pools for CPU-intensive and memory-intensive workloads</li>
<li>Use node affinity to schedule arms appropriately</li>
<li>Code example:
<pre><code class="language-yaml"># terraform/gke-node-pools.tf
resource "google_container_node_pool" "cpu_optimized" {
  name       = "cpu-optimized-pool"
  cluster    = google_container_cluster.octollm.name
  node_count = 2

  autoscaling {
    min_node_count = 2
    max_node_count = 20
  }

  node_config {
    machine_type = "n2-highcpu-16"  # 16 vCPU, 16 GB RAM

    labels = {
      workload-type = "cpu-optimized"
    }

    taint {
      key    = "workload-type"
      value  = "cpu-optimized"
      effect = "NO_SCHEDULE"
    }
  }
}

resource "google_container_node_pool" "memory_optimized" {
  name       = "memory-optimized-pool"
  cluster    = google_container_cluster.octollm.name
  node_count = 2

  autoscaling {
    min_node_count = 2
    max_node_count = 10
  }

  node_config {
    machine_type = "n2-highmem-8"  # 8 vCPU, 64 GB RAM

    labels = {
      workload-type = "memory-optimized"
    }

    taint {
      key    = "workload-type"
      value  = "memory-optimized"
      effect = "NO_SCHEDULE"
    }
  }
}
</code></pre>
</li>
<li>Files to create: <code>terraform/gke-node-pools.tf</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Test Cluster Autoscaling</strong> (1 hour)</p>
<ul>
<li>Simulate load spike</li>
<li>Verify nodes added automatically</li>
<li>Verify nodes removed after scale-down</li>
<li>Files to create: <code>scripts/test_cluster_autoscaling.sh</code></li>
</ul>
</li>
</ul>
<h4 id="load-testing-4-hours"><a class="header" href="#load-testing-4-hours">Load Testing (4 hours)</a></h4>
<ul>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Create Load Test Suite</strong> (2 hours)</p>
<ul>
<li>Use k6 or Locust for load generation</li>
<li>Simulate realistic traffic patterns</li>
<li>Code example:
<pre><code class="language-javascript">// tests/load/autoscaling_test.js
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate } from 'k6/metrics';

const failureRate = new Rate('failed_requests');

export let options = {
  stages: [
    { duration: '2m', target: 10 },   // Ramp up to 10 users
    { duration: '5m', target: 10 },   // Steady state
    { duration: '2m', target: 50 },   // Spike to 50 users
    { duration: '5m', target: 50 },   // Hold spike
    { duration: '2m', target: 100 },  // Extreme spike
    { duration: '5m', target: 100 },  // Hold extreme spike
    { duration: '5m', target: 0 },    // Ramp down
  ],
  thresholds: {
    'failed_requests': ['rate&lt;0.01'],  // &lt;1% failure rate
    'http_req_duration': ['p(95)&lt;15000'],  // P95 latency &lt;15s
  },
};

const BASE_URL = 'http://octollm-gateway.octollm.svc.cluster.local';

export default function () {
  // Submit a task
  const payload = JSON.stringify({
    goal: 'Analyze this code for security vulnerabilities',
    constraints: {
      max_cost_tokens: 10000,
      max_time_seconds: 300
    },
    context: {
      code: 'def login(username, password):\n    query = f"SELECT * FROM users WHERE username=\'{username}\' AND password=\'{password}\'"'
    }
  });

  const params = {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': 'Bearer test-token-123'
    },
  };

  const response = http.post(`${BASE_URL}/tasks`, payload, params);

  check(response, {
    'status is 201': (r) =&gt; r.status === 201,
    'has task_id': (r) =&gt; r.json('task_id') !== undefined,
  }) || failureRate.add(1);

  sleep(1);
}
</code></pre>
</li>
<li>Files to create: <code>tests/load/autoscaling_test.js</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Run Load Tests</strong> (2 hours)</p>
<ul>
<li>Execute load tests against staging environment</li>
<li>Monitor autoscaling behavior</li>
<li>Verify SLA compliance (99.9% uptime, &lt;15s P95 latency)</li>
<li>Generate load test report</li>
<li>Code example:
<pre><code class="language-bash"># scripts/run_load_test.sh
#!/bin/bash
set -e

echo "Starting autoscaling load test..."

# Run k6 load test
k6 run --out json=load_test_results.json tests/load/autoscaling_test.js

# Analyze results
python scripts/analyze_load_test.py load_test_results.json

# Check HPA events
echo "\n=== HPA Events ==="
kubectl get events -n octollm --field-selector involvedObject.kind=HorizontalPodAutoscaler

# Check pod scaling timeline
echo "\n=== Pod Count Timeline ==="
kubectl get pods -n octollm -l app=orchestrator --watch

echo "Load test complete. Review load_test_results.json for detailed metrics."
</code></pre>
</li>
<li>Files to create: <code>scripts/run_load_test.sh</code>, <code>scripts/analyze_load_test.py</code></li>
</ul>
</li>
</ul>
<h3 id="testing-requirements"><a class="header" href="#testing-requirements">Testing Requirements</a></h3>
<h4 id="unit-tests"><a class="header" href="#unit-tests">Unit Tests</a></h4>
<ul>
<li><input disabled="" type="checkbox"/>
HPA configuration validation (5 test cases)</li>
<li><input disabled="" type="checkbox"/>
VPA policy validation (5 test cases)</li>
<li><input disabled="" type="checkbox"/>
Custom metrics exporter (10 test cases)</li>
</ul>
<h4 id="integration-tests"><a class="header" href="#integration-tests">Integration Tests</a></h4>
<ul>
<li><input disabled="" type="checkbox"/>
HPA scaling behavior (scale up, scale down, flapping prevention)</li>
<li><input disabled="" type="checkbox"/>
VPA resource adjustment</li>
<li><input disabled="" type="checkbox"/>
Cluster autoscaler node provisioning</li>
<li><input disabled="" type="checkbox"/>
End-to-end autoscaling under load</li>
</ul>
<h4 id="performance-tests"><a class="header" href="#performance-tests">Performance Tests</a></h4>
<ul>
<li><input disabled="" type="checkbox"/>
Load test: 10x traffic spike (verify autoscaling handles without degradation)</li>
<li><input disabled="" type="checkbox"/>
Stress test: 100x traffic spike (verify graceful degradation)</li>
<li><input disabled="" type="checkbox"/>
Soak test: 24-hour sustained load (verify no memory leaks or resource drift)</li>
</ul>
<h3 id="documentation-deliverables"><a class="header" href="#documentation-deliverables">Documentation Deliverables</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
Autoscaling architecture diagram</li>
<li><input disabled="" type="checkbox"/>
HPA configuration guide</li>
<li><input disabled="" type="checkbox"/>
VPA tuning guide</li>
<li><input disabled="" type="checkbox"/>
Cluster autoscaler runbook</li>
<li><input disabled="" type="checkbox"/>
Load testing procedures</li>
<li><input disabled="" type="checkbox"/>
Troubleshooting guide (scaling issues)</li>
</ul>
<h3 id="success-criteria"><a class="header" href="#success-criteria">Success Criteria</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
HPA scales services within 60 seconds of load increase</li>
<li><input disabled="" type="checkbox"/>
VPA recommendations reduce resource waste by &gt;30%</li>
<li><input disabled="" type="checkbox"/>
Cluster autoscaler provisions nodes within 5 minutes</li>
<li><input disabled="" type="checkbox"/>
Load test passes with &lt;1% failure rate and P95 latency &lt;15s</li>
<li><input disabled="" type="checkbox"/>
Cost per task unchanged despite autoscaling overhead</li>
</ul>
<h3 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h3>
<ol>
<li><strong>HPA Flapping</strong>: Too aggressive scale-down causes constant scaling up/down—use longer stabilization windows</li>
<li><strong>VPA Disruption</strong>: Auto mode restarts pods—use recommendation mode for critical services</li>
<li><strong>Node Affinity Conflicts</strong>: Pods can't schedule if no matching nodes—ensure default node pool</li>
<li><strong>Custom Metrics Lag</strong>: Prometheus scrape interval causes scaling delays—reduce to 15s for autoscaling metrics</li>
<li><strong>Resource Limits</strong>: HPA can't scale if pods hit resource limits—ensure limits &gt; requests</li>
</ol>
<h3 id="estimated-effort"><a class="header" href="#estimated-effort">Estimated Effort</a></h3>
<ul>
<li>Development: 22 hours</li>
<li>Testing: 6 hours</li>
<li>Documentation: 3 hours</li>
<li><strong>Total</strong>: 31 hours (~2 weeks for 2 engineers)</li>
</ul>
<h3 id="dependencies"><a class="header" href="#dependencies">Dependencies</a></h3>
<ul>
<li><strong>Prerequisites</strong>: Phase 3 complete (Kubernetes deployment, monitoring stack)</li>
<li><strong>Blocking</strong>: None</li>
<li><strong>Blocked By</strong>: None</li>
</ul>
<hr />
<h2 id="sprint-62-cost-optimization-week-35-36"><a class="header" href="#sprint-62-cost-optimization-week-35-36">Sprint 6.2: Cost Optimization [Week 35-36]</a></h2>
<p><strong>Duration</strong>: 2 weeks
<strong>Team</strong>: 3 engineers (1 SRE, 1 ML engineer, 1 Python)
<strong>Prerequisites</strong>: Sprint 6.1 complete (autoscaling)
<strong>Priority</strong>: HIGH</p>
<h3 id="sprint-goals-1"><a class="header" href="#sprint-goals-1">Sprint Goals</a></h3>
<ul>
<li>Right-size all services based on actual usage</li>
<li>Implement spot/preemptible instances for non-critical workloads</li>
<li>Purchase reserved capacity for baseline load</li>
<li>Optimize LLM costs (prompt caching, smaller models, fine-tuning)</li>
<li>Implement request batching and deduplication</li>
<li>Reduce cost per task by 50% vs Phase 5</li>
</ul>
<h3 id="architecture-decisions-1"><a class="header" href="#architecture-decisions-1">Architecture Decisions</a></h3>
<p><strong>Compute</strong>: Mix of on-demand (20%), spot instances (60%), reserved capacity (20%)
<strong>LLM Strategy</strong>: Use cheapest model per task type (GPT-3.5 for simple, GPT-4 for complex)
<strong>Caching</strong>: Aggressive prompt caching with semantic similarity matching
<strong>Batching</strong>: Batch similar requests to reduce LLM API overhead
<strong>Fine-Tuning</strong>: Fine-tune smaller models (Mistral 7B) to replace GPT-3.5 for common patterns</p>
<h3 id="tasks-1"><a class="header" href="#tasks-1">Tasks</a></h3>
<h4 id="right-sizing-8-hours"><a class="header" href="#right-sizing-8-hours">Right-Sizing (8 hours)</a></h4>
<ul>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Analyze Resource Usage</strong> (3 hours)</p>
<ul>
<li>Use VPA recommendations and Prometheus metrics</li>
<li>Identify over-provisioned services</li>
<li>Code example:
<pre><code class="language-python"># scripts/analyze_resource_usage.py
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Any

class ResourceAnalyzer:
    """Analyze resource usage and identify optimization opportunities."""

    def __init__(self, prometheus_url: str):
        self.prometheus_url = prometheus_url

    def analyze_service(
        self,
        service_name: str,
        days_lookback: int = 30
    ) -&gt; Dict[str, Any]:
        """Analyze resource usage for a service."""

        end_time = datetime.now()
        start_time = end_time - timedelta(days=days_lookback)

        # Query CPU usage
        cpu_query = f'''
            avg_over_time(
                rate(container_cpu_usage_seconds_total{{
                    namespace="octollm",
                    pod=~"{service_name}-.*"
                }}[5m])[{days_lookback}d:5m]
            )
        '''

        cpu_usage = self._query_prometheus(cpu_query)

        # Query memory usage
        memory_query = f'''
            avg_over_time(
                container_memory_working_set_bytes{{
                    namespace="octollm",
                    pod=~"{service_name}-.*"
                }}[{days_lookback}d:5m]
            )
        '''

        memory_usage = self._query_prometheus(memory_query)

        # Get current resource requests
        current_requests = self._get_current_requests(service_name)

        # Calculate waste
        cpu_waste_percent = (
            (current_requests['cpu'] - cpu_usage['p95']) /
            current_requests['cpu'] * 100
        )

        memory_waste_percent = (
            (current_requests['memory'] - memory_usage['p95']) /
            current_requests['memory'] * 100
        )

        return {
            'service': service_name,
            'current_cpu_request': current_requests['cpu'],
            'p95_cpu_usage': cpu_usage['p95'],
            'cpu_waste_percent': cpu_waste_percent,
            'current_memory_request': current_requests['memory'],
            'p95_memory_usage': memory_usage['p95'],
            'memory_waste_percent': memory_waste_percent,
            'recommendation': self._generate_recommendation(
                current_requests,
                cpu_usage,
                memory_usage
            )
        }

    def _query_prometheus(self, query: str) -&gt; Dict[str, float]:
        """Query Prometheus and return percentile statistics."""
        # Implementation: Call Prometheus API, calculate percentiles
        pass

    def _get_current_requests(self, service_name: str) -&gt; Dict[str, float]:
        """Get current resource requests from Kubernetes."""
        # Implementation: Call Kubernetes API
        pass

    def _generate_recommendation(
        self,
        current: Dict[str, float],
        cpu_usage: Dict[str, float],
        memory_usage: Dict[str, float]
    ) -&gt; str:
        """Generate right-sizing recommendation."""

        # Add 20% buffer to P95 usage for headroom
        recommended_cpu = cpu_usage['p95'] * 1.2
        recommended_memory = memory_usage['p95'] * 1.2

        if recommended_cpu &lt; current['cpu'] * 0.8:
            return f"Reduce CPU request to {recommended_cpu:.2f} cores"
        elif recommended_cpu &gt; current['cpu'] * 1.2:
            return f"Increase CPU request to {recommended_cpu:.2f} cores"

        if recommended_memory &lt; current['memory'] * 0.8:
            return f"Reduce memory request to {recommended_memory / 1e9:.2f} GB"
        elif recommended_memory &gt; current['memory'] * 1.2:
            return f"Increase memory request to {recommended_memory / 1e9:.2f} GB"

        return "Current sizing is appropriate"
</code></pre>
</li>
<li>Files to create: <code>scripts/analyze_resource_usage.py</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Apply Right-Sizing</strong> (2 hours)</p>
<ul>
<li>Update resource requests/limits for all services</li>
<li>Deploy changes incrementally</li>
<li>Monitor for performance regressions</li>
<li>Files to update: All deployment YAML files</li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Calculate Cost Savings</strong> (1 hour)</p>
<ul>
<li>Compare costs before/after right-sizing</li>
<li>Generate cost savings report</li>
<li>Files to create: <code>docs/cost-optimization/right-sizing-report.md</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Set Up Cost Monitoring Dashboard</strong> (2 hours)</p>
<ul>
<li>Grafana dashboard for cost tracking</li>
<li>Alert on cost anomalies</li>
<li>Code example:
<pre><code class="language-json">{
  "dashboard": {
    "title": "OctoLLM Cost Monitoring",
    "panels": [
      {
        "title": "Total Monthly Cost",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(kube_pod_container_resource_requests{namespace='octollm'} * on(node) group_left() node_cost_hourly) * 730"
          }
        ]
      },
      {
        "title": "Cost by Service",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum by (pod) (kube_pod_container_resource_requests{namespace='octollm'} * on(node) group_left() node_cost_hourly) * 730"
          }
        ]
      },
      {
        "title": "LLM API Costs",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(llm_cost_usd_total)"
          }
        ]
      }
    ]
  }
}
</code></pre>
</li>
<li>Files to create: <code>k8s/monitoring/grafana-dashboards/cost-monitoring.json</code></li>
</ul>
</li>
</ul>
<h4 id="spot-instances-6-hours"><a class="header" href="#spot-instances-6-hours">Spot Instances (6 hours)</a></h4>
<ul>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Create Spot Instance Node Pool</strong> (2 hours)</p>
<ul>
<li>Configure with appropriate labels and taints</li>
<li>Set up fallback to on-demand if spot unavailable</li>
<li>Code example:
<pre><code class="language-yaml"># terraform/gke-spot-node-pool.tf
resource "google_container_node_pool" "spot_pool" {
  name       = "spot-pool"
  cluster    = google_container_cluster.octollm.name
  node_count = 5

  autoscaling {
    min_node_count = 3
    max_node_count = 50
  }

  node_config {
    machine_type = "n2-standard-8"
    spot         = true  # Preemptible/spot instance

    labels = {
      workload-type = "spot"
    }

    taint {
      key    = "workload-type"
      value  = "spot"
      effect = "NO_SCHEDULE"
    }

    metadata = {
      disable-legacy-endpoints = "true"
    }
  }
}
</code></pre>
</li>
<li>Files to create: <code>terraform/gke-spot-node-pool.tf</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Configure Services for Spot Tolerance</strong> (3 hours)</p>
<ul>
<li>Add node affinity to prefer spot instances</li>
<li>Implement graceful shutdown for preemption</li>
<li>Add PodDisruptionBudgets to ensure availability</li>
<li>Code example:
<pre><code class="language-yaml"># k8s/arms/executor-deployment.yaml (updated for spot)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: executor-arm
  namespace: octollm
spec:
  replicas: 5
  template:
    spec:
      # Prefer spot instances, fallback to on-demand
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: workload-type
                operator: In
                values:
                - spot

      tolerations:
      - key: workload-type
        operator: Equal
        value: spot
        effect: NoSchedule

      # Graceful shutdown for preemption
      terminationGracePeriodSeconds: 60

      containers:
      - name: executor-arm
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 30"]  # Drain connections
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: executor-arm-pdb
  namespace: octollm
spec:
  minAvailable: 2  # Ensure at least 2 replicas always available
  selector:
    matchLabels:
      app: executor-arm
</code></pre>
</li>
<li>Files to update: All arm deployment YAML files</li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Test Spot Instance Preemption</strong> (1 hour)</p>
<ul>
<li>Simulate preemption events</li>
<li>Verify graceful failover</li>
<li>Files to create: <code>scripts/test_spot_preemption.sh</code></li>
</ul>
</li>
</ul>
<h4 id="llm-cost-optimization-10-hours"><a class="header" href="#llm-cost-optimization-10-hours">LLM Cost Optimization (10 hours)</a></h4>
<ul>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Implement Prompt Caching</strong> (4 hours)</p>
<ul>
<li>Cache LLM responses with semantic similarity matching</li>
<li>Use vector embeddings to find similar prompts</li>
<li>Code example:
<pre><code class="language-python"># orchestrator/llm/cached_client.py
from openai import AsyncOpenAI
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
from typing import Dict, Any, Optional, List
import hashlib
import json

class CachedLLMClient:
    """LLM client with semantic caching."""

    def __init__(
        self,
        openai_client: AsyncOpenAI,
        qdrant_client: QdrantClient,
        embedding_model: SentenceTransformer,
        similarity_threshold: float = 0.95,
        collection_name: str = "llm_cache"
    ):
        self.openai = openai_client
        self.qdrant = qdrant_client
        self.embedding_model = embedding_model
        self.similarity_threshold = similarity_threshold
        self.collection_name = collection_name

        # Create collection if not exists
        self._init_collection()

    def _init_collection(self):
        """Initialize Qdrant collection for cache."""
        from qdrant_client.models import Distance, VectorParams

        try:
            self.qdrant.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=384,  # all-MiniLM-L6-v2 embedding size
                    distance=Distance.COSINE
                )
            )
        except Exception:
            pass  # Collection already exists

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = "gpt-4-turbo-preview",
        temperature: float = 0.0,
        **kwargs
    ) -&gt; Dict[str, Any]:
        """Create chat completion with semantic caching."""

        # Create cache key from messages
        prompt = self._messages_to_text(messages)
        cache_key = self._create_cache_key(prompt, model, temperature)

        # Check exact match cache first (fast)
        exact_match = await self._check_exact_cache(cache_key)
        if exact_match:
            return exact_match

        # Check semantic similarity cache (slower)
        if temperature == 0.0:  # Only use semantic cache for deterministic requests
            semantic_match = await self._check_semantic_cache(prompt, model)
            if semantic_match:
                return semantic_match

        # Cache miss - call LLM
        response = await self.openai.chat.completions.create(
            messages=messages,
            model=model,
            temperature=temperature,
            **kwargs
        )

        # Store in cache
        await self._store_in_cache(cache_key, prompt, model, response)

        return response.model_dump()

    def _messages_to_text(self, messages: List[Dict[str, str]]) -&gt; str:
        """Convert messages to single text for embedding."""
        return "\n".join(f"{m['role']}: {m['content']}" for m in messages)

    def _create_cache_key(
        self,
        prompt: str,
        model: str,
        temperature: float
    ) -&gt; str:
        """Create deterministic cache key."""
        key_input = f"{prompt}|{model}|{temperature}"
        return hashlib.sha256(key_input.encode()).hexdigest()

    async def _check_exact_cache(self, cache_key: str) -&gt; Optional[Dict[str, Any]]:
        """Check Redis for exact cache hit."""
        # Implementation: Query Redis
        pass

    async def _check_semantic_cache(
        self,
        prompt: str,
        model: str
    ) -&gt; Optional[Dict[str, Any]]:
        """Check Qdrant for semantically similar cached responses."""

        # Generate embedding
        embedding = self.embedding_model.encode(prompt).tolist()

        # Search for similar prompts
        results = self.qdrant.search(
            collection_name=self.collection_name,
            query_vector=embedding,
            limit=1,
            score_threshold=self.similarity_threshold,
            query_filter={
                "must": [
                    {"key": "model", "match": {"value": model}}
                ]
            }
        )

        if results and results[0].score &gt;= self.similarity_threshold:
            # Cache hit
            cached_response = results[0].payload["response"]
            return json.loads(cached_response)

        return None

    async def _store_in_cache(
        self,
        cache_key: str,
        prompt: str,
        model: str,
        response: Any
    ):
        """Store response in both exact and semantic caches."""

        # Store in Redis (exact match)
        # Implementation: Store in Redis with TTL

        # Store in Qdrant (semantic similarity)
        embedding = self.embedding_model.encode(prompt).tolist()

        self.qdrant.upsert(
            collection_name=self.collection_name,
            points=[
                {
                    "id": cache_key,
                    "vector": embedding,
                    "payload": {
                        "prompt": prompt,
                        "model": model,
                        "response": json.dumps(response.model_dump()),
                        "timestamp": datetime.utcnow().isoformat()
                    }
                }
            ]
        )
</code></pre>
</li>
<li>Files to create: <code>orchestrator/llm/cached_client.py</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Implement Model Selection Strategy</strong> (3 hours)</p>
<ul>
<li>Route to cheapest model capable of solving task</li>
<li>Use complexity classifier to determine required model</li>
<li>Code example:
<pre><code class="language-python"># orchestrator/llm/model_selector.py
from typing import Dict, Any, List
import re

class ModelSelector:
    """Select cheapest LLM model for a given task."""

    # Cost per 1M tokens (input/output)
    MODEL_COSTS = {
        "gpt-4-turbo-preview": (10.00, 30.00),
        "gpt-4": (30.00, 60.00),
        "gpt-3.5-turbo": (0.50, 1.50),
        "mistral-7b-instruct": (0.20, 0.20),  # Self-hosted
    }

    # Model capabilities
    MODEL_CAPABILITIES = {
        "gpt-4-turbo-preview": {"reasoning": 10, "coding": 9, "knowledge": 10},
        "gpt-4": {"reasoning": 10, "coding": 10, "knowledge": 10},
        "gpt-3.5-turbo": {"reasoning": 7, "coding": 7, "knowledge": 8},
        "mistral-7b-instruct": {"reasoning": 6, "coding": 6, "knowledge": 6},
    }

    def select_model(
        self,
        task_description: str,
        required_capability: str = "reasoning",
        min_capability_score: int = 7
    ) -&gt; str:
        """Select cheapest model meeting requirements."""

        # Determine task complexity
        complexity = self._assess_complexity(task_description)

        # Filter models by capability
        suitable_models = [
            model for model, capabilities in self.MODEL_CAPABILITIES.items()
            if capabilities.get(required_capability, 0) &gt;= min(complexity, min_capability_score)
        ]

        if not suitable_models:
            # Fallback to most capable model
            return "gpt-4-turbo-preview"

        # Select cheapest suitable model
        cheapest = min(
            suitable_models,
            key=lambda m: sum(self.MODEL_COSTS[m])
        )

        return cheapest

    def _assess_complexity(self, task_description: str) -&gt; int:
        """Assess task complexity (1-10 scale)."""

        complexity_indicators = {
            # High complexity
            r"multi-step|complex|advanced|intricate": 9,
            r"requires.*reasoning|logical.*deduction": 8,
            r"analyze|evaluate|compare": 7,

            # Medium complexity
            r"explain|describe|summarize": 6,
            r"translate|convert|transform": 5,

            # Low complexity
            r"list|enumerate|identify": 4,
            r"yes|no|true|false": 3,
            r"simple|basic|straightforward": 2,
        }

        max_complexity = 5  # Default medium complexity
        for pattern, score in complexity_indicators.items():
            if re.search(pattern, task_description, re.IGNORECASE):
                max_complexity = max(max_complexity, score)

        return max_complexity
</code></pre>
</li>
<li>Files to create: <code>orchestrator/llm/model_selector.py</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Fine-Tune Specialist Models</strong> (3 hours)</p>
<ul>
<li>Collect training data from task logs</li>
<li>Fine-tune Mistral 7B for common patterns</li>
<li>Replace GPT-3.5 calls with fine-tuned model</li>
<li>Code example:
<pre><code class="language-python"># scripts/fine_tune_specialist.py
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from typing import List, Dict, Any
import json

class SpecialistModelTrainer:
    """Fine-tune specialist models for common tasks."""

    def __init__(self, base_model: str = "mistralai/Mistral-7B-Instruct-v0.2"):
        self.base_model = base_model
        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            load_in_4bit=True,  # QLoRA for efficient fine-tuning
            device_map="auto"
        )

    def prepare_training_data(
        self,
        task_logs_path: str,
        task_type: str
    ) -&gt; Dataset:
        """Prepare training data from task logs."""

        # Load task logs
        with open(task_logs_path) as f:
            logs = [json.loads(line) for line in f]

        # Filter by task type
        relevant_logs = [
            log for log in logs
            if log.get("task_type") == task_type
        ]

        # Format for instruction tuning
        training_examples = []
        for log in relevant_logs:
            training_examples.append({
                "instruction": log["input_prompt"],
                "output": log["llm_response"]
            })

        return Dataset.from_list(training_examples)

    def fine_tune(
        self,
        dataset: Dataset,
        output_dir: str,
        num_epochs: int = 3
    ):
        """Fine-tune model on dataset."""

        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            learning_rate=2e-5,
            warmup_steps=100,
            logging_steps=10,
            save_steps=100,
            evaluation_strategy="steps",
            eval_steps=100,
            load_best_model_at_end=True
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            tokenizer=self.tokenizer
        )

        trainer.train()
        trainer.save_model(output_dir)

if __name__ == "__main__":
    trainer = SpecialistModelTrainer()

    # Fine-tune for code review task
    dataset = trainer.prepare_training_data(
        task_logs_path="logs/task_logs.jsonl",
        task_type="code_review"
    )

    trainer.fine_tune(
        dataset=dataset,
        output_dir="models/mistral-7b-code-review"
    )
</code></pre>
</li>
<li>Files to create: <code>scripts/fine_tune_specialist.py</code></li>
</ul>
</li>
</ul>
<h4 id="request-optimization-4-hours"><a class="header" href="#request-optimization-4-hours">Request Optimization (4 hours)</a></h4>
<ul>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Implement Request Batching</strong> (2 hours)</p>
<ul>
<li>Batch similar requests to reduce API overhead</li>
<li>Use async processing with batch windows</li>
<li>Files to create: <code>orchestrator/llm/batch_processor.py</code></li>
</ul>
</li>
<li>
<p><input disabled="" type="checkbox"/>
<strong>Implement Request Deduplication</strong> (2 hours)</p>
<ul>
<li>Detect duplicate requests in flight</li>
<li>Return cached result to duplicate requesters</li>
<li>Files to create: <code>orchestrator/middleware/deduplication.py</code></li>
</ul>
</li>
</ul>
<h3 id="testing-requirements-1"><a class="header" href="#testing-requirements-1">Testing Requirements</a></h3>
<h4 id="unit-tests-1"><a class="header" href="#unit-tests-1">Unit Tests</a></h4>
<ul>
<li><input disabled="" type="checkbox"/>
Resource analyzer calculations (10 test cases)</li>
<li><input disabled="" type="checkbox"/>
Model selector logic (15 test cases)</li>
<li><input disabled="" type="checkbox"/>
Prompt caching (20 test cases)</li>
<li><input disabled="" type="checkbox"/>
Request batching (10 test cases)</li>
</ul>
<h4 id="integration-tests-1"><a class="header" href="#integration-tests-1">Integration Tests</a></h4>
<ul>
<li><input disabled="" type="checkbox"/>
End-to-end cost tracking</li>
<li><input disabled="" type="checkbox"/>
Spot instance failover</li>
<li><input disabled="" type="checkbox"/>
LLM cost reduction verification</li>
<li><input disabled="" type="checkbox"/>
Fine-tuned model accuracy vs base model</li>
</ul>
<h4 id="performance-tests-1"><a class="header" href="#performance-tests-1">Performance Tests</a></h4>
<ul>
<li><input disabled="" type="checkbox"/>
Cost per task benchmark (before/after optimization)</li>
<li><input disabled="" type="checkbox"/>
Cache hit rate measurement (target &gt;60%)</li>
<li><input disabled="" type="checkbox"/>
Fine-tuned model latency vs GPT-3.5</li>
</ul>
<h3 id="documentation-deliverables-1"><a class="header" href="#documentation-deliverables-1">Documentation Deliverables</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
Cost optimization strategy guide</li>
<li><input disabled="" type="checkbox"/>
Right-sizing procedures</li>
<li><input disabled="" type="checkbox"/>
Spot instance configuration guide</li>
<li><input disabled="" type="checkbox"/>
LLM cost reduction techniques</li>
<li><input disabled="" type="checkbox"/>
Fine-tuning runbooks</li>
</ul>
<h3 id="success-criteria-1"><a class="header" href="#success-criteria-1">Success Criteria</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
Cost per task reduced by 50% vs Phase 5</li>
<li><input disabled="" type="checkbox"/>
Resource waste reduced by &gt;30%</li>
<li><input disabled="" type="checkbox"/>
LLM cache hit rate &gt;60%</li>
<li><input disabled="" type="checkbox"/>
Fine-tuned models achieve &gt;95% accuracy of GPT-3.5 on target tasks</li>
<li><input disabled="" type="checkbox"/>
Zero performance degradation from cost optimizations</li>
</ul>
<h3 id="common-pitfalls-1"><a class="header" href="#common-pitfalls-1">Common Pitfalls</a></h3>
<ol>
<li><strong>Over-Optimization</strong>: Aggressive right-sizing causes OOM kills—maintain 20% buffer</li>
<li><strong>Spot Instance Unavailability</strong>: Spot capacity shortages in peak hours—keep on-demand fallback</li>
<li><strong>Cache Staleness</strong>: Cached responses become outdated—implement TTL and versioning</li>
<li><strong>Fine-Tuning Overfitting</strong>: Model only works on training distribution—use diverse dataset</li>
<li><strong>Premature Optimization</strong>: Optimize before understanding usage patterns—collect 30+ days data first</li>
</ol>
<h3 id="estimated-effort-1"><a class="header" href="#estimated-effort-1">Estimated Effort</a></h3>
<ul>
<li>Development: 28 hours</li>
<li>Testing: 6 hours</li>
<li>Documentation: 3 hours</li>
<li><strong>Total</strong>: 37 hours (~2 weeks for 3 engineers)</li>
</ul>
<h3 id="dependencies-1"><a class="header" href="#dependencies-1">Dependencies</a></h3>
<ul>
<li><strong>Prerequisites</strong>: Sprint 6.1 (autoscaling), Phase 3 (monitoring)</li>
<li><strong>Blocking</strong>: None</li>
<li><strong>Blocked By</strong>: None</li>
</ul>
<hr />
<h2 id="sprint-63-compliance-implementation-week-37-38"><a class="header" href="#sprint-63-compliance-implementation-week-37-38">Sprint 6.3: Compliance Implementation [Week 37-38]</a></h2>
<p><strong>(Abbreviated for space - full version would be 1,200-1,500 lines)</strong></p>
<h3 id="sprint-goals-2"><a class="header" href="#sprint-goals-2">Sprint Goals</a></h3>
<ul>
<li>Achieve SOC 2 Type II compliance</li>
<li>Implement ISO 27001 controls</li>
<li>Ensure GDPR compliance (data protection, right to erasure)</li>
<li>Ensure CCPA compliance (opt-out, data disclosure)</li>
<li>HIPAA readiness (encryption, access controls, audit logs)</li>
<li>Pass external compliance audits</li>
</ul>
<h3 id="key-tasks-summary"><a class="header" href="#key-tasks-summary">Key Tasks (Summary)</a></h3>
<ol>
<li>
<p><strong>SOC 2 Type II Preparation</strong> (12 hours)</p>
<ul>
<li>Implement security controls (TSC)</li>
<li>Document policies and procedures</li>
<li>Conduct internal audit</li>
<li>Contract external auditor</li>
</ul>
</li>
<li>
<p><strong>ISO 27001 Implementation</strong> (10 hours)</p>
<ul>
<li>Risk assessment and treatment</li>
<li>Information security policies</li>
<li>Access control procedures</li>
<li>Incident management</li>
</ul>
</li>
<li>
<p><strong>GDPR Compliance</strong> (8 hours)</p>
<ul>
<li>Data protection impact assessment (DPIA)</li>
<li>Consent management</li>
<li>Right to erasure implementation</li>
<li>Data portability</li>
</ul>
</li>
<li>
<p><strong>CCPA Compliance</strong> (6 hours)</p>
<ul>
<li>Consumer rights implementation (opt-out, disclosure)</li>
<li>Privacy policy updates</li>
<li>Data inventory and mapping</li>
</ul>
</li>
<li>
<p><strong>HIPAA Readiness</strong> (6 hours)</p>
<ul>
<li>Encryption at rest and in transit</li>
<li>Access controls and audit logs</li>
<li>Business associate agreements (BAA)</li>
<li>Breach notification procedures</li>
</ul>
</li>
</ol>
<h3 id="estimated-effort-42-hours-2-weeks-for-2-engineers"><a class="header" href="#estimated-effort-42-hours-2-weeks-for-2-engineers">Estimated Effort: 42 hours (~2 weeks for 2 engineers)</a></h3>
<hr />
<h2 id="sprint-64-advanced-performance-week-39-40"><a class="header" href="#sprint-64-advanced-performance-week-39-40">Sprint 6.4: Advanced Performance [Week 39-40]</a></h2>
<p><strong>(Abbreviated for space - full version would be 1,200-1,500 lines)</strong></p>
<h3 id="sprint-goals-3"><a class="header" href="#sprint-goals-3">Sprint Goals</a></h3>
<ul>
<li>Rewrite performance-critical components in Rust</li>
<li>Fine-tune LLM models for specific tasks</li>
<li>Implement advanced caching strategies (multi-tier, predictive)</li>
<li>Add speculative execution for anticipated tasks</li>
<li>Achieve P99 latency &lt;10s (vs &lt;30s in Phase 1)</li>
<li>Reduce LLM API costs by additional 30%</li>
</ul>
<h3 id="key-tasks-summary-1"><a class="header" href="#key-tasks-summary-1">Key Tasks (Summary)</a></h3>
<ol>
<li>
<p><strong>Rust Performance Rewrites</strong> (16 hours)</p>
<ul>
<li>Rewrite Planner Arm in Rust (2x faster)</li>
<li>Rewrite Judge Arm in Rust (3x faster)</li>
<li>Optimize Reflex Layer (target &lt;5ms P95)</li>
</ul>
</li>
<li>
<p><strong>Model Fine-Tuning</strong> (12 hours)</p>
<ul>
<li>Fine-tune task decomposition model</li>
<li>Fine-tune code generation model</li>
<li>Fine-tune validation model</li>
<li>Deploy fine-tuned models</li>
</ul>
</li>
<li>
<p><strong>Advanced Caching</strong> (10 hours)</p>
<ul>
<li>Multi-tier caching (L1: Redis, L2: Qdrant, L3: S3)</li>
<li>Predictive cache warming</li>
<li>Cache invalidation strategies</li>
</ul>
</li>
<li>
<p><strong>Speculative Execution</strong> (8 hours)</p>
<ul>
<li>Predict next likely task based on patterns</li>
<li>Precompute results in background</li>
<li>Serve from cache when requested</li>
</ul>
</li>
<li>
<p><strong>Performance Benchmarking</strong> (4 hours)</p>
<ul>
<li>Comprehensive performance test suite</li>
<li>Compare Phase 6 vs Phase 1 metrics</li>
<li>Latency reduction verification</li>
</ul>
</li>
</ol>
<h3 id="estimated-effort-50-hours-25-weeks-for-2-engineers"><a class="header" href="#estimated-effort-50-hours-25-weeks-for-2-engineers">Estimated Effort: 50 hours (~2.5 weeks for 2 engineers)</a></h3>
<hr />
<h2 id="sprint-65-multi-tenancy-week-41-42"><a class="header" href="#sprint-65-multi-tenancy-week-41-42">Sprint 6.5: Multi-Tenancy [Week 41-42]</a></h2>
<p><strong>(Abbreviated for space - full version would be 1,200-1,500 lines)</strong></p>
<h3 id="sprint-goals-4"><a class="header" href="#sprint-goals-4">Sprint Goals</a></h3>
<ul>
<li>Implement tenant isolation (network, storage, compute)</li>
<li>Add authentication and authorization per tenant</li>
<li>Implement usage-based billing</li>
<li>Create tenant management portal</li>
<li>Test multi-tenant security isolation</li>
<li>Document multi-tenancy architecture</li>
</ul>
<h3 id="key-tasks-summary-2"><a class="header" href="#key-tasks-summary-2">Key Tasks (Summary)</a></h3>
<ol>
<li>
<p><strong>Tenant Isolation</strong> (12 hours)</p>
<ul>
<li>Kubernetes namespaces per tenant</li>
<li>Network policies for isolation</li>
<li>Separate database schemas</li>
<li>Qdrant collections per tenant</li>
</ul>
</li>
<li>
<p><strong>Authentication and Authorization</strong> (10 hours)</p>
<ul>
<li>Multi-tenant Auth0 integration</li>
<li>Tenant-scoped API keys</li>
<li>Role-based access control (RBAC) per tenant</li>
</ul>
</li>
<li>
<p><strong>Usage-Based Billing</strong> (10 hours)</p>
<ul>
<li>Meter API calls, LLM tokens, compute time</li>
<li>Integrate with Stripe for billing</li>
<li>Generate invoices and usage reports</li>
</ul>
</li>
<li>
<p><strong>Tenant Management Portal</strong> (8 hours)</p>
<ul>
<li>React admin dashboard</li>
<li>Tenant provisioning and configuration</li>
<li>Usage analytics and billing</li>
</ul>
</li>
<li>
<p><strong>Security Testing</strong> (6 hours)</p>
<ul>
<li>Tenant isolation verification</li>
<li>Cross-tenant access attempts (should all fail)</li>
<li>Data leakage testing</li>
</ul>
</li>
</ol>
<h3 id="estimated-effort-46-hours-25-weeks-for-2-engineers"><a class="header" href="#estimated-effort-46-hours-25-weeks-for-2-engineers">Estimated Effort: 46 hours (~2.5 weeks for 2 engineers)</a></h3>
<hr />
<h2 id="phase-6-summary"><a class="header" href="#phase-6-summary">Phase 6 Summary</a></h2>
<p><strong>Total Tasks</strong>: 80+ production readiness tasks across 5 sprints
<strong>Estimated Duration</strong>: 8-10 weeks with 4-5 engineers
<strong>Total Estimated Hours</strong>: ~206 hours development + ~40 hours testing + ~25 hours documentation = 271 hours</p>
<p><strong>Deliverables</strong>:</p>
<ul>
<li>Autoscaling infrastructure (HPA, VPA, cluster autoscaler)</li>
<li>50% cost reduction vs Phase 5</li>
<li>SOC 2 Type II, ISO 27001, GDPR, CCPA compliance</li>
<li>P99 latency &lt;10s (67% improvement vs Phase 1)</li>
<li>Multi-tenant production platform</li>
</ul>
<p><strong>Completion Checklist</strong>:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Autoscaling handles 10x traffic spikes</li>
<li><input disabled="" type="checkbox"/>
Cost per task reduced by 50%</li>
<li><input disabled="" type="checkbox"/>
SOC 2 Type II audit passed</li>
<li><input disabled="" type="checkbox"/>
P99 latency &lt;10s achieved</li>
<li><input disabled="" type="checkbox"/>
Multi-tenant isolation verified</li>
<li><input disabled="" type="checkbox"/>
Production SLA: 99.9% uptime, &lt;15s P95 latency</li>
<li><input disabled="" type="checkbox"/>
Zero security incidents in first 90 days</li>
<li><input disabled="" type="checkbox"/>
Public API and documentation published</li>
</ul>
<p><strong>Next Steps</strong>: Production launch and customer onboarding</p>
<hr />
<p><strong>Document Version</strong>: 1.0
<strong>Last Updated</strong>: 2025-11-10
<strong>Maintained By</strong>: OctoLLM Production Team</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../project-tracking/phases/phase-5.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../../project-tracking/status.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../project-tracking/phases/phase-5.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../../project-tracking/status.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../../elasticlunr.min.js"></script>
        <script src="../../mark.min.js"></script>
        <script src="../../searcher.js"></script>

        <script src="../../clipboard.min.js"></script>
        <script src="../../highlight.js"></script>
        <script src="../../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
