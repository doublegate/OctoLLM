<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Judge Arm - OctoLLM Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Distributed AI Architecture for Offensive Security and Developer Tooling - Comprehensive technical documentation covering architecture, API, development, operations, and security.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">OctoLLM Documentation</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/doublegate/OctoLLM" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/doublegate/OctoLLM/edit/main/docs/src/components/arms/judge-arm.md" title="Suggest an edit" aria-label="Suggest an edit" rel="edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="judge-arm-validation--quality-assurance"><a class="header" href="#judge-arm-validation--quality-assurance">Judge Arm: Validation &amp; Quality Assurance</a></h1>
<p><strong>Components</strong> &gt; <strong>Arms</strong> &gt; Judge Arm</p>
<p><strong>Version</strong>: 1.0
<strong>Technology</strong>: Python 3.11+ / FastAPI
<strong>Cost Tier</strong>: 2 (Medium)
<strong>Average Latency</strong>: 0.5-2 seconds
<strong>Status</strong>: Phase 1 Complete</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#core-functionality">Core Functionality</a>
<ul>
<li><a href="#validation-types">Validation Types</a></li>
<li><a href="#multi-layer-validation">Multi-Layer Validation</a></li>
<li><a href="#acceptance-criteria-checking">Acceptance Criteria Checking</a></li>
<li><a href="#hallucination-detection">Hallucination Detection</a></li>
</ul>
</li>
<li><a href="#validation-layers">Validation Layers</a>
<ul>
<li><a href="#layer-1-schema-validation">Layer 1: Schema Validation</a></li>
<li><a href="#layer-2-fact-checking">Layer 2: Fact-Checking</a></li>
<li><a href="#layer-3-criteria-validation">Layer 3: Criteria Validation</a></li>
<li><a href="#layer-4-hallucination-detection">Layer 4: Hallucination Detection</a></li>
<li><a href="#layer-5-quality-assessment">Layer 5: Quality Assessment</a></li>
</ul>
</li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#judgearm-class">JudgeArm Class</a></li>
<li><a href="#schema-validator">Schema Validator</a></li>
<li><a href="#fact-checker">Fact Checker</a></li>
<li><a href="#quality-assessor">Quality Assessor</a></li>
</ul>
</li>
<li><a href="#api-specification">API Specification</a>
<ul>
<li><a href="#validate-output">Validate Output</a></li>
<li><a href="#response-formats">Response Formats</a></li>
</ul>
</li>
<li><a href="#data-models">Data Models</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#performance-characteristics">Performance Characteristics</a></li>
<li><a href="#testing">Testing</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#see-also">See Also</a></li>
</ul>
<hr />
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The Judge Arm is responsible for validating outputs from other arms against acceptance criteria, checking facts, detecting hallucinations, and ensuring quality standards. It acts as the quality assurance gate before results are returned to the orchestrator.</p>
<h3 id="key-features"><a class="header" href="#key-features">Key Features</a></h3>
<ul>
<li><strong>Multi-Layer Validation</strong>: Five distinct validation layers</li>
<li><strong>Schema Validation</strong>: JSON/data structure compliance</li>
<li><strong>Fact-Checking</strong>: Verify claims against trusted sources</li>
<li><strong>Criteria Checking</strong>: Ensure acceptance criteria are met</li>
<li><strong>Hallucination Detection</strong>: Identify unsupported or fabricated information</li>
<li><strong>Quality Assessment</strong>: General quality scoring</li>
<li><strong>Confidence Scoring</strong>: Quantify validation certainty</li>
<li><strong>Issue Classification</strong>: Errors, warnings, and informational suggestions</li>
</ul>
<h3 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h3>
<ol>
<li><strong>Defense in Depth</strong>: Multiple independent validation layers</li>
<li><strong>Fail-Safe</strong>: Errors result in rejection</li>
<li><strong>Explainability</strong>: Clear issue descriptions with suggestions</li>
<li><strong>Severity Levels</strong>: Distinguish critical errors from warnings</li>
<li><strong>Confidence Quantification</strong>: Express uncertainty in results</li>
</ol>
<hr />
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<pre><code class="language-mermaid">graph TB
    subgraph "Judge Arm"
        API[API Endpoint]
        PROC[Request Processor]
        COORD[Validation Coordinator]
    end

    subgraph "Validation Layers"
        SCHEMA[Schema Validator]
        FACTS[Fact Checker]
        CRITERIA[Criteria Evaluator]
        HALLUC[Hallucination Detector]
        QUALITY[Quality Assessor]
    end

    subgraph "External Services"
        LLM[LLM for Evaluation]
        SOURCES[Trusted Sources]
        KB[Knowledge Base]
    end

    ORCH[Orchestrator] --&gt;|Validate Request| API
    API --&gt; PROC
    PROC --&gt; COORD

    COORD --&gt; SCHEMA
    COORD --&gt; FACTS
    COORD --&gt; CRITERIA
    COORD --&gt; HALLUC
    COORD --&gt; QUALITY

    SCHEMA --&gt;|Schema Issues| COORD
    FACTS --&gt; SOURCES
    FACTS --&gt; KB
    FACTS --&gt;|Fact Issues| COORD

    CRITERIA --&gt; LLM
    CRITERIA --&gt;|Criteria Issues| COORD

    HALLUC --&gt; LLM
    HALLUC --&gt;|Hallucination Issues| COORD

    QUALITY --&gt; LLM
    QUALITY --&gt;|Quality Issues| COORD

    COORD --&gt;|Validation Result| API
    API --&gt;|Pass/Fail| ORCH

    style COORD fill:#ff9,stroke:#333
    style ORCH fill:#9f9,stroke:#333
    style LLM fill:#f9f,stroke:#333
</code></pre>
<h3 id="validation-flow"><a class="header" href="#validation-flow">Validation Flow</a></h3>
<pre><code class="language-mermaid">sequenceDiagram
    participant O as Orchestrator
    participant J as Judge Arm
    participant S as Schema Validator
    participant F as Fact Checker
    participant C as Criteria Evaluator
    participant H as Hallucination Detector
    participant Q as Quality Assessor

    O-&gt;&gt;J: Validate output

    par Layer 1: Schema
        J-&gt;&gt;S: Validate structure
        S--&gt;&gt;J: Schema issues
    and Layer 2: Facts
        J-&gt;&gt;F: Check facts
        F--&gt;&gt;J: Fact issues
    and Layer 3: Criteria
        J-&gt;&gt;C: Evaluate criteria
        C--&gt;&gt;J: Criteria results
    and Layer 4: Hallucinations
        J-&gt;&gt;H: Detect hallucinations
        H--&gt;&gt;J: Hallucination issues
    and Layer 5: Quality
        J-&gt;&gt;Q: Assess quality
        Q--&gt;&gt;J: Quality score
    end

    J-&gt;&gt;J: Aggregate results
    J-&gt;&gt;J: Calculate confidence

    alt Valid (no errors)
        J--&gt;&gt;O: ValidationResult (valid=true)
    else Invalid (has errors)
        J--&gt;&gt;O: ValidationResult (valid=false)
    end
</code></pre>
<hr />
<h2 id="core-functionality"><a class="header" href="#core-functionality">Core Functionality</a></h2>
<h3 id="validation-types"><a class="header" href="#validation-types">Validation Types</a></h3>
<pre><code class="language-python">from enum import Enum

class ValidationType(str, Enum):
    SCHEMA = "schema"                    # JSON/data structure validation
    FACTS = "facts"                      # Fact-checking against sources
    CRITERIA = "criteria"                # Acceptance criteria checking
    QUALITY = "quality"                  # General quality assessment
    HALLUCINATION = "hallucination"      # Detect false information
</code></pre>
<h3 id="multi-layer-validation"><a class="header" href="#multi-layer-validation">Multi-Layer Validation</a></h3>
<p>The Judge Arm performs validation through five independent layers, each producing issues with severity levels:</p>
<div class="table-wrapper"><table><thead><tr><th>Severity</th><th>Meaning</th><th>Impact</th></tr></thead><tbody>
<tr><td><strong>error</strong></td><td>Critical problem, must fix</td><td><code>valid = false</code></td></tr>
<tr><td><strong>warning</strong></td><td>Potential issue, review recommended</td><td><code>valid = true</code> (if no errors)</td></tr>
<tr><td><strong>info</strong></td><td>Suggestion for improvement</td><td><code>valid = true</code></td></tr>
</tbody></table>
</div>
<h3 id="acceptance-criteria-checking"><a class="header" href="#acceptance-criteria-checking">Acceptance Criteria Checking</a></h3>
<p>Evaluates whether output meets specified requirements using LLM-based assessment:</p>
<pre><code class="language-python">async def _check_criteria(
    self,
    output: Any,
    criteria: List[str]
) -&gt; CriteriaResult:
    """Check if output meets acceptance criteria."""

    passed = []
    failed = []
    issues = []

    for criterion in criteria:
        # Use LLM to evaluate criterion
        is_met = await self._evaluate_criterion(output, criterion)

        if is_met:
            passed.append(criterion)
        else:
            failed.append(criterion)
            issues.append(ValidationIssue(
                severity="error",
                type="criteria_not_met",
                message=f"Acceptance criterion not met: {criterion}",
                suggestion="Review output and ensure it addresses this requirement"
            ))

    confidence = len(passed) / len(criteria) if criteria else 1.0

    return CriteriaResult(
        passed=passed,
        failed=failed,
        issues=issues,
        confidence=confidence
    )
</code></pre>
<h3 id="hallucination-detection"><a class="header" href="#hallucination-detection">Hallucination Detection</a></h3>
<p>Identifies claims not supported by provided context:</p>
<pre><code class="language-python">async def _detect_hallucinations(
    self,
    output: Any,
    context: Dict[str, Any]
) -&gt; HallucinationResult:
    """Detect unsupported claims or fabricated information."""

    # Extract claims from output
    claims = await self._extract_claims(output)

    issues = []
    hallucination_count = 0

    for claim in claims:
        # Check if claim is supported by context
        is_supported = await self._verify_claim_support(claim, context)

        if not is_supported:
            hallucination_count += 1
            issues.append(ValidationIssue(
                severity="warning",
                type="unsupported_claim",
                message=f"Claim not supported by context: {claim}",
                suggestion="Verify this information or mark as uncertain"
            ))

    confidence = 1.0 - (hallucination_count / len(claims)) if claims else 1.0

    return HallucinationResult(
        issues=issues,
        confidence=confidence,
        hallucination_count=hallucination_count,
        total_claims=len(claims)
    )
</code></pre>
<hr />
<h2 id="validation-layers"><a class="header" href="#validation-layers">Validation Layers</a></h2>
<h3 id="layer-1-schema-validation"><a class="header" href="#layer-1-schema-validation">Layer 1: Schema Validation</a></h3>
<p>Validates data structure against JSON Schema or Pydantic models:</p>
<pre><code class="language-python">class SchemaValidator:
    """Validate output against expected schema."""

    async def validate(
        self,
        output: Any,
        schema: Dict[str, Any]
    ) -&gt; ValidationResult:
        """Validate output structure."""

        try:
            # Use jsonschema for validation
            import jsonschema
            jsonschema.validate(output, schema)

            return ValidationResult(
                issues=[],
                confidence=1.0
            )

        except jsonschema.ValidationError as e:
            return ValidationResult(
                issues=[
                    ValidationIssue(
                        severity="error",
                        type="schema_violation",
                        message=f"Schema validation failed: {e.message}",
                        location=".".join(str(p) for p in e.path),
                        suggestion="Ensure output matches expected structure"
                    )
                ],
                confidence=0.0
            )
</code></pre>
<h3 id="layer-2-fact-checking"><a class="header" href="#layer-2-fact-checking">Layer 2: Fact-Checking</a></h3>
<p>Verifies factual claims against trusted sources:</p>
<pre><code class="language-python">class FactChecker:
    """Verify facts against trusted sources."""

    def __init__(self, knowledge_base_url: str):
        self.kb_url = knowledge_base_url

    async def verify_facts(
        self,
        output: Any,
        trusted_sources: List[str]
    ) -&gt; ValidationResult:
        """Check facts against trusted sources."""

        # Extract factual statements
        facts = await self._extract_facts(output)

        issues = []
        verified_count = 0

        for fact in facts:
            # Query knowledge base
            is_verified = await self._verify_fact(fact, trusted_sources)

            if not is_verified:
                issues.append(ValidationIssue(
                    severity="warning",
                    type="unverified_fact",
                    message=f"Cannot verify fact: {fact}",
                    suggestion="Provide source or mark as unverified"
                ))
            else:
                verified_count += 1

        confidence = verified_count / len(facts) if facts else 1.0

        return ValidationResult(
            issues=issues,
            confidence=confidence
        )
</code></pre>
<h3 id="layer-3-criteria-validation"><a class="header" href="#layer-3-criteria-validation">Layer 3: Criteria Validation</a></h3>
<p>LLM-based evaluation of acceptance criteria:</p>
<pre><code class="language-python">async def _evaluate_criterion(self, output: Any, criterion: str) -&gt; bool:
    """Evaluate if output meets criterion using LLM."""

    prompt = f"""Evaluate if the following output meets this criterion:

Criterion: {criterion}

Output:
{json.dumps(output, indent=2)}

Respond with ONLY "YES" if the criterion is met, or "NO" if not met.
"""

    response = await openai.ChatCompletion.acreate(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a precise evaluator."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.0,
        max_tokens=10
    )

    answer = response.choices[0].message.content.strip().upper()
    return answer == "YES"
</code></pre>
<h3 id="layer-4-hallucination-detection"><a class="header" href="#layer-4-hallucination-detection">Layer 4: Hallucination Detection</a></h3>
<p>Extracts and verifies claims:</p>
<pre><code class="language-python">async def _extract_claims(self, output: Any) -&gt; List[str]:
    """Extract factual claims from output."""

    prompt = f"""Extract all factual claims from this output as a JSON array:

Output:
{json.dumps(output, indent=2)}

Return only verifiable factual statements, not opinions or instructions.
Format: ["claim 1", "claim 2", ...]
"""

    response = await openai.ChatCompletion.acreate(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a fact extractor."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.0,
        max_tokens=500
    )

    content = response.choices[0].message.content.strip()
    claims = json.loads(content)
    return claims

async def _verify_claim_support(
    self,
    claim: str,
    context: Dict[str, Any]
) -&gt; bool:
    """Verify if claim is supported by context."""

    prompt = f"""Is this claim supported by the provided context?

Claim: {claim}

Context:
{json.dumps(context, indent=2)}

Respond with ONLY "YES" if supported, "NO" if not.
"""

    response = await openai.ChatCompletion.acreate(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a claim verifier."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.0,
        max_tokens=10
    )

    answer = response.choices[0].message.content.strip().upper()
    return answer == "YES"
</code></pre>
<h3 id="layer-5-quality-assessment"><a class="header" href="#layer-5-quality-assessment">Layer 5: Quality Assessment</a></h3>
<p>General quality scoring:</p>
<pre><code class="language-python">class QualityAssessor:
    """Assess overall quality of output."""

    async def assess(self, output: Any) -&gt; QualityResult:
        """Perform comprehensive quality assessment."""

        issues = []
        scores = []

        # Check completeness
        completeness = await self._check_completeness(output)
        scores.append(completeness.score)
        issues.extend(completeness.issues)

        # Check clarity
        clarity = await self._check_clarity(output)
        scores.append(clarity.score)
        issues.extend(clarity.issues)

        # Check consistency
        consistency = await self._check_consistency(output)
        scores.append(consistency.score)
        issues.extend(consistency.issues)

        overall_score = sum(scores) / len(scores)

        return QualityResult(
            score=overall_score,
            issues=issues
        )
</code></pre>
<hr />
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<h3 id="judgearm-class"><a class="header" href="#judgearm-class">JudgeArm Class</a></h3>
<pre><code class="language-python">from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field

class ValidationRequest(BaseModel):
    output: Any = Field(..., description="Output to validate")
    validation_types: List[ValidationType]
    acceptance_criteria: List[str] = Field(default_factory=list)
    expected_schema: Optional[Dict[str, Any]] = None
    trusted_sources: List[str] = Field(default_factory=list)
    context: Dict[str, Any] = Field(default_factory=dict)

class ValidationIssue(BaseModel):
    severity: str = Field(..., description="error, warning, info")
    type: str
    message: str
    location: Optional[str] = None
    suggestion: Optional[str] = None

class ValidationResult(BaseModel):
    valid: bool
    confidence: float = Field(..., ge=0.0, le=1.0)
    issues: List[ValidationIssue] = Field(default_factory=list)
    passed_criteria: List[str] = Field(default_factory=list)
    failed_criteria: List[str] = Field(default_factory=list)
    quality_score: float = Field(..., ge=0.0, le=1.0)
    metadata: Dict[str, Any] = Field(default_factory=dict)

class JudgeArm:
    """Output validation and quality assurance specialist."""

    def __init__(self):
        self.schema_validator = SchemaValidator()
        self.fact_checker = FactChecker()
        self.quality_assessor = QualityAssessor()

    async def validate(self, req: ValidationRequest) -&gt; ValidationResult:
        """Validate output through multiple layers."""

        issues = []
        passed_criteria = []
        failed_criteria = []
        confidence_scores = []

        # Layer 1: Schema validation
        if ValidationType.SCHEMA in req.validation_types and req.expected_schema:
            schema_result = await self.schema_validator.validate(
                req.output,
                req.expected_schema
            )
            issues.extend(schema_result.issues)
            confidence_scores.append(schema_result.confidence)

        # Layer 2: Fact-checking
        if ValidationType.FACTS in req.validation_types:
            fact_result = await self.fact_checker.verify_facts(
                req.output,
                req.trusted_sources
            )
            issues.extend(fact_result.issues)
            confidence_scores.append(fact_result.confidence)

        # Layer 3: Acceptance criteria
        if ValidationType.CRITERIA in req.validation_types:
            criteria_result = await self._check_criteria(
                req.output,
                req.acceptance_criteria
            )
            passed_criteria = criteria_result.passed
            failed_criteria = criteria_result.failed
            issues.extend(criteria_result.issues)
            confidence_scores.append(criteria_result.confidence)

        # Layer 4: Hallucination detection
        if ValidationType.HALLUCINATION in req.validation_types:
            hallucination_result = await self._detect_hallucinations(
                req.output,
                req.context
            )
            issues.extend(hallucination_result.issues)
            confidence_scores.append(hallucination_result.confidence)

        # Layer 5: Quality assessment
        if ValidationType.QUALITY in req.validation_types:
            quality_result = await self.quality_assessor.assess(req.output)
            issues.extend(quality_result.issues)
            confidence_scores.append(quality_result.score)

        # Determine overall validity
        has_errors = any(issue.severity == "error" for issue in issues)
        valid = not has_errors and len(failed_criteria) == 0

        # Calculate overall confidence
        overall_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.5

        return ValidationResult(
            valid=valid,
            confidence=overall_confidence,
            issues=issues,
            passed_criteria=passed_criteria,
            failed_criteria=failed_criteria,
            quality_score=quality_result.score if quality_result else 0.5,
            metadata={
                "validation_types_run": [vt.value for vt in req.validation_types],
                "total_issues": len(issues),
                "error_count": sum(1 for i in issues if i.severity == "error"),
                "warning_count": sum(1 for i in issues if i.severity == "warning")
            }
        )
</code></pre>
<h3 id="schema-validator"><a class="header" href="#schema-validator">Schema Validator</a></h3>
<p>See <a href="#layer-1-schema-validation">Layer 1: Schema Validation</a> section.</p>
<h3 id="fact-checker"><a class="header" href="#fact-checker">Fact Checker</a></h3>
<p>See <a href="#layer-2-fact-checking">Layer 2: Fact-Checking</a> section.</p>
<h3 id="quality-assessor"><a class="header" href="#quality-assessor">Quality Assessor</a></h3>
<p>See <a href="#layer-5-quality-assessment">Layer 5: Quality Assessment</a> section.</p>
<hr />
<h2 id="api-specification"><a class="header" href="#api-specification">API Specification</a></h2>
<h3 id="validate-output"><a class="header" href="#validate-output">Validate Output</a></h3>
<p><strong>Endpoint</strong>: <code>POST /validate</code></p>
<p><strong>Request Body</strong>:</p>
<pre><code class="language-json">{
  "output": {
    "code": "def sort_list(lst): return sorted(lst)",
    "tests": "assert sort_list([3,1,2]) == [1,2,3]"
  },
  "validation_types": ["schema", "criteria", "quality"],
  "acceptance_criteria": [
    "Code implements sorting functionality",
    "Tests are included",
    "Function has proper naming"
  ],
  "expected_schema": {
    "type": "object",
    "required": ["code", "tests"],
    "properties": {
      "code": {"type": "string"},
      "tests": {"type": "string"}
    }
  }
}
</code></pre>
<p><strong>Field Descriptions</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>output</code></td><td>any</td><td>Yes</td><td>Output to validate</td></tr>
<tr><td><code>validation_types</code></td><td>array[string]</td><td>Yes</td><td>Types of validation to perform</td></tr>
<tr><td><code>acceptance_criteria</code></td><td>array[string]</td><td>No</td><td>Criteria that must be met</td></tr>
<tr><td><code>expected_schema</code></td><td>object</td><td>No</td><td>JSON Schema for structure validation</td></tr>
<tr><td><code>trusted_sources</code></td><td>array[string]</td><td>No</td><td>URLs of trusted sources for fact-checking</td></tr>
<tr><td><code>context</code></td><td>object</td><td>No</td><td>Context for hallucination detection</td></tr>
</tbody></table>
</div>
<h3 id="response-formats"><a class="header" href="#response-formats">Response Formats</a></h3>
<p><strong>Valid Output</strong> (200 OK):</p>
<pre><code class="language-json">{
  "valid": true,
  "confidence": 0.92,
  "issues": [
    {
      "severity": "info",
      "type": "style_suggestion",
      "message": "Consider adding docstring to function",
      "location": "function:sort_list",
      "suggestion": "Add docstring explaining parameters and return value"
    }
  ],
  "passed_criteria": [
    "Code implements sorting functionality",
    "Tests are included",
    "Function has proper naming"
  ],
  "failed_criteria": [],
  "quality_score": 0.85,
  "metadata": {
    "validation_types_run": ["schema", "criteria", "quality"],
    "total_issues": 1,
    "error_count": 0,
    "warning_count": 0
  }
}
</code></pre>
<p><strong>Invalid Output</strong> (200 OK with valid=false):</p>
<pre><code class="language-json">{
  "valid": false,
  "confidence": 0.45,
  "issues": [
    {
      "severity": "error",
      "type": "schema_violation",
      "message": "Missing required field 'tests'",
      "location": "root",
      "suggestion": "Add 'tests' field to output"
    },
    {
      "severity": "error",
      "type": "criteria_not_met",
      "message": "Acceptance criterion not met: Tests are included",
      "suggestion": "Review output and ensure it addresses this requirement"
    },
    {
      "severity": "warning",
      "type": "unsupported_claim",
      "message": "Claim not supported by context: Function is O(n log n) complexity",
      "suggestion": "Verify this information or mark as uncertain"
    }
  ],
  "passed_criteria": [
    "Code implements sorting functionality"
  ],
  "failed_criteria": [
    "Tests are included",
    "Function has proper naming"
  ],
  "quality_score": 0.60,
  "metadata": {
    "validation_types_run": ["schema", "criteria", "hallucination", "quality"],
    "total_issues": 3,
    "error_count": 2,
    "warning_count": 1
  }
}
</code></pre>
<hr />
<h2 id="data-models"><a class="header" href="#data-models">Data Models</a></h2>
<h3 id="request-models"><a class="header" href="#request-models">Request Models</a></h3>
<pre><code class="language-python">class CriteriaResult(BaseModel):
    passed: List[str]
    failed: List[str]
    issues: List[ValidationIssue]
    confidence: float

class HallucinationResult(BaseModel):
    issues: List[ValidationIssue]
    confidence: float
    hallucination_count: int
    total_claims: int

class QualityResult(BaseModel):
    score: float
    issues: List[ValidationIssue]
</code></pre>
<hr />
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<h3 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h3>
<pre><code class="language-bash"># Judge Arm Configuration
JUDGE_PORT=8005
JUDGE_MODEL=gpt-3.5-turbo
JUDGE_TEMPERATURE=0.0

# Knowledge Base
KNOWLEDGE_BASE_URL=http://postgres:5432
TRUSTED_SOURCES_URL=http://retriever-arm:8006

# Validation Settings
ENABLE_HALLUCINATION_DETECTION=true
ENABLE_FACT_CHECKING=true
FACT_CHECK_THRESHOLD=0.8
QUALITY_MIN_SCORE=0.7

# Logging
LOG_LEVEL=info
LOG_VALIDATION_RESULTS=true
</code></pre>
<hr />
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h2>
<h3 id="latency"><a class="header" href="#latency">Latency</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Validation Type</th><th>P50</th><th>P95</th><th>P99</th></tr></thead><tbody>
<tr><td>Schema</td><td>10ms</td><td>20ms</td><td>50ms</td></tr>
<tr><td>Facts</td><td>500ms</td><td>1s</td><td>2s</td></tr>
<tr><td>Criteria</td><td>800ms</td><td>1.5s</td><td>3s</td></tr>
<tr><td>Hallucination</td><td>1s</td><td>2s</td><td>4s</td></tr>
<tr><td>Quality</td><td>500ms</td><td>1s</td><td>2s</td></tr>
<tr><td><strong>Total (all)</strong></td><td><strong>2s</strong></td><td><strong>4s</strong></td><td><strong>8s</strong></td></tr>
</tbody></table>
</div>
<h3 id="accuracy"><a class="header" href="#accuracy">Accuracy</a></h3>
<ul>
<li><strong>Schema Validation</strong>: 100% (deterministic)</li>
<li><strong>Fact-Checking</strong>: 75-85% (depends on sources)</li>
<li><strong>Criteria Evaluation</strong>: 80-90% (LLM-based)</li>
<li><strong>Hallucination Detection</strong>: 70-80% (context-dependent)</li>
<li><strong>Quality Assessment</strong>: 75-85% (subjective)</li>
</ul>
<hr />
<h2 id="testing"><a class="header" href="#testing">Testing</a></h2>
<h3 id="unit-tests"><a class="header" href="#unit-tests">Unit Tests</a></h3>
<pre><code class="language-python">import pytest
from judge_arm import JudgeArm, ValidationRequest, ValidationType

@pytest.fixture
def judge():
    return JudgeArm()

@pytest.mark.asyncio
async def test_schema_validation(judge):
    request = ValidationRequest(
        output={"code": "def test(): pass"},
        validation_types=[ValidationType.SCHEMA],
        expected_schema={
            "type": "object",
            "required": ["code"],
            "properties": {"code": {"type": "string"}}
        }
    )

    result = await judge.validate(request)

    assert result.valid
    assert result.confidence &gt; 0.9
    assert len(result.issues) == 0

@pytest.mark.asyncio
async def test_criteria_checking(judge):
    request = ValidationRequest(
        output={"code": "def sort(lst): return sorted(lst)"},
        validation_types=[ValidationType.CRITERIA],
        acceptance_criteria=[
            "Code implements sorting",
            "Function is named 'sort'"
        ]
    )

    result = await judge.validate(request)

    assert len(result.passed_criteria) == 2
    assert len(result.failed_criteria) == 0
</code></pre>
<hr />
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<h3 id="dockerfile"><a class="header" href="#dockerfile">Dockerfile</a></h3>
<pre><code class="language-dockerfile">FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY judge_arm/ ./judge_arm/

RUN useradd -m -u 1000 judge &amp;&amp; chown -R judge:judge /app
USER judge

ENV PYTHONUNBUFFERED=1
EXPOSE 8005

CMD ["uvicorn", "judge_arm.main:app", "--host", "0.0.0.0", "--port", "8005"]
</code></pre>
<h3 id="kubernetes-deployment"><a class="header" href="#kubernetes-deployment">Kubernetes Deployment</a></h3>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: judge-arm
  namespace: octollm
spec:
  replicas: 2
  selector:
    matchLabels:
      app: judge-arm
  template:
    metadata:
      labels:
        app: judge-arm
    spec:
      containers:
      - name: judge
        image: octollm/judge-arm:1.0
        ports:
        - containerPort: 8005
        env:
        - name: JUDGE_PORT
          value: "8005"
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: openai-credentials
              key: api-key
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
</code></pre>
<hr />
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="../orchestrator.html">Orchestrator Component</a> - Coordinates validation</li>
<li><a href="./coder-arm.html">Coder Arm</a> - Code generation that requires validation</li>
<li><a href="./planner-arm.html">Planner Arm</a> - Plan validation</li>
<li><a href="./guardian-arm.html">Safety Guardian Arm</a> - Pre-execution security validation</li>
<li><a href="../../api/rest-api.html">API Reference</a> - Complete API documentation</li>
</ul>
<hr />
<p><strong>Document Status</strong>: Phase 1 Complete
<strong>Last Updated</strong>: 2025-11-10
<strong>Maintainer</strong>: OctoLLM Core Team
<strong>Next Review</strong>: 2025-12-10</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../components/arms/coder-arm.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../../components/arms/guardian-arm.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../components/arms/coder-arm.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../../components/arms/guardian-arm.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../../elasticlunr.min.js"></script>
        <script src="../../mark.min.js"></script>
        <script src="../../searcher.js"></script>

        <script src="../../clipboard.min.js"></script>
        <script src="../../highlight.js"></script>
        <script src="../../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
